{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f2c5a412-3ff4-4d48-a48b-c306ff7639ff",
     "showTitle": false,
     "title": ""
    },
    "id": "xU7S8_kzC7Cd"
   },
   "source": [
    "# Machine Learning und Softcomputing Assessment\n",
    "#### **Sarkasmuserkennung mittels Kernel Ridge Regression**\n",
    "*Die Ergebnisse dieses Notebooks wurden im Rahmen des Assessments Machine Learning and Softcomputing erarbeitet.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f1db978f-b2f2-4d8a-bfdf-92074244ad69",
     "showTitle": false,
     "title": ""
    },
    "id": "9l5KpFk1C7Cg"
   },
   "source": [
    "### Aufgabenstellung\n",
    "Das Themengebiet **Erkennung von Sarkasmus in Text** ist ein häufig erforschtes Thema. Es fällt in die Kategorisierung **Semantische Textklassifizierung**, welche bereits mehrere Ansätze enthält, um Features aus Texten zu extrahieren. Die Aufgabe stellt sich zusammen aus der Entwicklung eines von qualitativen Features aus einem selbst ausgewählten Datensatz, zum trainieren eines Modells, welches im Anschluss die Texteingabe nach Sarkasmus oder nicht Sarkasmus klassifiziert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "321e162e-e8f4-4b3c-bd70-4c0b709f61ab",
     "showTitle": false,
     "title": ""
    },
    "id": "MFmGJeviC7Ch"
   },
   "source": [
    "### Formelverzeichnis\n",
    "\n",
    "|Variablen|Beschreibung|\n",
    "|---------|------------|\n",
    "|𝑤|Gewichtsmatrix/-vektor\n",
    "|𝑥|Datenpunkt (Training)\n",
    "|z|Datenpunkt (Input)\n",
    "|𝑏|lineare Konstante\n",
    "|α|Korrekturwert\n",
    "|𝜙(𝑥)|Feature-Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "569ea7e2-1434-4718-9352-ed525eb4a0eb",
     "showTitle": false,
     "title": ""
    },
    "id": "bVzEJJsiC7Ci"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "### 1. Einführung in Kernel Ridge Regression\n",
    "Kernel Ridge Regression (KRR) ist ein Verfahren der Support Vector Machines (SVM), bei dem die klassische Regression oder Klassifikation um die L2-Regularisierung und Kernels erweitert wird. Durch die Kernels kann das Verfahren den sogenannten Kernel-Trick nutzen, um Rechenzeit einzuspaaren. Kernel Ridge Regression verhält sich um Vergleich zu klassischen Support Vector Machines effizienter und genauer für Datensätze mit einer ` Datenpunktanzahl < 25.000 `. Zudem können hochdimensionale Feature-Vektoren durch den Kernel-Trick wesentlich effizienter oder gar überhaupt genutzt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ce4704f-15fb-45dd-aba9-b385025cbaf7",
     "showTitle": false,
     "title": ""
    },
    "id": "kEr4-a7yC7Ci"
   },
   "source": [
    "<a name=\"1.1\"></a>\n",
    "#### 1.1. SVM mittles Kerneln\n",
    "Eine klassiche lineare Support Vector Machine prognostiziert Ergebnisse anhand der Formel: \n",
    "\n",
    "> `𝑓(z) = <𝑤, z> + 𝑏`\n",
    "\n",
    "Da die Datenpunkte in der Regel hochdimensional sind, entsprechen 𝑤 und 𝑥 Vektoren oder Matrizen, für die das innere Produkt gebildet werden muss, um eine Multiplikation zu erreichen. Die klassische Formel, bzw. die Gewichtsmatrix kann durch den sog. Perzeptron-Algorithmus weiter aufgeschlüsselt werden.\n",
    "\n",
    "> `w = ∑ 𝛼 * 𝑦 * 𝑥)`\n",
    "\n",
    "Der Perzeptron-Algorithmus betrachtet hierbei alle Trainingsdatenpunkte und ihre Ergebnisse und kombiniert diese mit einen Korrekturwert. Somit können die Gewichte iterativ angepasst werden und verlieren dennoch nicht an Aussagekraft im Training durch das zwischenspeichern der Trainingsdaten.<br><br>\n",
    "Weiterführend wird das sog. Feature-Mapping angewandt. Das Feature-Mapping ist eine unbekannte Funktion, die einen Datenpunkt in höhere Dimensionen überführt. Dies ist vorallem wichtig, um noch nicht herausgestellte Zusammenhänge abzubilden. Das Feature-Mapping lässt sich hierbei auf die, aus dem Perzeptron-Algorithmus resultierende, Interpolationfunktion einbinden. Hierzu wird das innere Produkt des Gewichts- und Inputdatenverktors aufgeschlüsselt. Der Gewichtsvektor wird ersetzt und lässt eine Verknüpfung der Trainingsdaten mit den Inputdaten zu (inneres Produkt). Auf die Datenpunkte des inneren Produktes lässt sich anschließend das Feature-Mapping anwenden.\n",
    "\n",
    "> `𝑓(𝑥) = ∑ 𝛼 * 𝑦 * <𝜙(𝑥),𝜙(z)> + 𝑏`\n",
    "\n",
    "Das hierbei entstandene innere Produkt lässt sich nun durch die Einführung des sog. Kernels ersetzen. Ein Kernel bildet dieses innere Produkt mit den Feature-Mapping ab.\n",
    "\n",
    "> `𝑓(𝑥) = ∑ 𝛼 * 𝑦 * 𝑘(𝑥,z) + 𝑏`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "21b7eaa0-1f03-46ab-993c-47a91e1afa87",
     "showTitle": false,
     "title": ""
    },
    "id": "Qbg5ADrxC7Cj"
   },
   "source": [
    "#### 1.2. Kernel-Trick\n",
    "Der Kernel-Trick bezieht sich auf die im vorangegangenen Schritt eingeführten Kernel in den SVM-Funktionen. Durch den Trick können die inneren Produkte mittels vordefinierten Kernel-Methoden im Input-Space durchgeführt werden. Hiervon profitiert voallem die Laufzeit und Speicherkomplexität, da die Umwandlung (Feature-Mapping) in den Feature-Space entfällt. \n",
    "\n",
    "> **Linearer Kernel:**  `<𝑥,𝑦>`<br>\n",
    "> **Polynominaler Kernel:**  `(<𝑥,𝑦>+ 𝑐)^𝑑`<br>\n",
    "\n",
    "\n",
    "> **Achtung:** Hierbei muss jedoch darauf geachtet werden, dass die Ersetzung der Umwandlung nur für bestimmte Funktionen möglich ist. Konkret gilt [Mercer's Theorem](https://)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "db3852a3-eb8d-4545-b2b7-b80c201477ec",
     "showTitle": false,
     "title": ""
    },
    "id": "ml2Ewt27C7Ck"
   },
   "source": [
    "#### 1.3. L2-Regularisierung\n",
    "Die L2-Regularisierung ist ein anderer Begriff für die Ridge Regression, die in diesem Verfahren in Kombination mit dem Kernel-Trick angewandt wird. \n",
    "\n",
    "Es gibt verschiedene Verfahren der Regression, die bei verschiedenen Szenarien eingesetzt werden können. Die Bekannteste ist dabei wohl die einfache lineare Regression, welche eine statische Methode ist, die die Beziehung zwischen einer Antwortvariablen ` y ` und einer Prädiktorvariablen ` x ` beschreibt.\n",
    "\n",
    "> ` y = a + b * x `\n",
    "\n",
    "Wenn in einem Modell mehrere Prädiktorvariablen vorkommen, die jeweils voneinander unabhängig sind und eine Beziehung zu der Antwortvariablen haben, wird die multiple lineare Regression eingesetzt. Es gibt dabei wieder eine Antwortvariable ` y ` und ` p ` Prädiktorvariablen, die mit ` x(p) ` beschrieben werden. Hinzukommt kommt ` β(p) `, welche den durschnittlichen Effekt einer Zunahme von ` x(p) ` beschreibt, wenn eine Einheit auf ` y ` gerechnet wird. Alle andere Prädiktoren werden dabei festgehalten. \n",
    "\n",
    "> ` y = β(0) + β(1) * x(1) + ... + β(p) * x(p) + ε ` \n",
    ">\n",
    "> **Minimierung von** ` RSS`\n",
    "\n",
    "Da es jedoch bei der multiplen linearen Regression zu Multikollinarität kommen kann, wenn zwei oder mehrere Prädiktorvariablen miteinander in Beziehung stehen, wird die Ridge Regression eingesetzt. Die multiple lineare Regression versucht, die Summe der quadratischen Residuen zu minimieren. \n",
    "\n",
    "Bei der Ridge Regression kommt ein Strafterm hinzu und wird minimiert. Dieser hängt besonders von dem Hyperparameter ` λ ` ab. Wenn ` λ=0 ` ist, dann gibt es keine Veränderung. Je größer ` λ ` jedoch wird, desto einflussreicher ist ihr Schrumpfungsgrad. ALlgemeien schrumpfen die Prädiktorvariablen, die am wenigsten Einfluss haben, am schnellsten gegen 0. \n",
    "\n",
    "> **Minimierung von** ` RSS + λ * ∑ * β(j)² `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c9310b6-7c03-4b4c-a07b-8e245898383a",
     "showTitle": false,
     "title": ""
    },
    "id": "QnfQJOF8C7Cl"
   },
   "source": [
    "### 2. Auswahl des Datensatzes\n",
    "Anhand einer vorangegangen Internetrecherche sind wir auf die folgenden drei Datensätze gestoßen, die aus unterschiedlichen Erhebungsstandorten eine Klassifikation nach Sarkasmus und nicht Sarkasmus enthielten.\n",
    "Außerdem ist es wichtig, dass die Datensätze bereits balanciert wurden, ansonsten muss ein weiterer Balancierungsprozess vorgeschoben werden.\n",
    "\n",
    "Nach der Evaluation der Vor- und Nachteile der Datensätze, kamen wir zum Ergebnis, dass der Datensatz **Sarcasm on Reddit** am repräsentativsten den modernen Sarkasmus darlegt.\n",
    "<br><br>\n",
    "\n",
    "Datensatz|Vorteile|Nachteile\n",
    "---------|--------|---------\n",
    "[iSarcasm](https://)||\n",
    "[Sarcasm on Reddit](https://)|- modernes Sarkasmusverständnis<br>- große Anzahl an Datenpunkten|- viele numerische Kontexte<br>- viele Abkürzungen\n",
    "[News Headlines Sarcasm](https://)||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "54e91f27-6f08-43e9-9b2f-f00ae5fdabfa",
     "showTitle": false,
     "title": ""
    },
    "id": "3scoNZ0rC7Cl"
   },
   "source": [
    "#### 2.1. *Sarcasm on Reddit* Metadaten\n",
    ">**Metadaten** = Grundlegende Informationen über die Einträge des Datensatzes.\n",
    "\n",
    "Metainformation|Wert\n",
    "---------------|----\n",
    "Anzahl an Datenpunkten|~1.000.000 Einträge\n",
    "Anzahl valider Datenpunkte|962.295 Einträge\n",
    "Ausgeglichenheitsfaktor|50% Sarkasmus, 50% Kein Sarkasmus\n",
    "Hauptspalten|\"label\", \"comment\", \"parent_comment\"\n",
    "Nebenspalten|\"author\", \"subreddit\", \"score\", \"ups\", \"down\", \"date\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "67dcfb60-7aee-4d2e-a2b8-eeb6727686a9",
     "showTitle": false,
     "title": ""
    },
    "id": "T0EKv9ddC7Cm"
   },
   "source": [
    "---\n",
    "> ### Information zu Databricks:\n",
    "> Um Daten aus dem DBFS zu löschen, kann der folgende Befehl ausgeführt werden. Wichtig ist, dass erst alle Dateien aus einem Verzeichnis gelöscht werden müssen, bevor ein Verzeichnis selbst gelöscht werden kann.\n",
    ">```\n",
    ">dbutils.fs.rm(\"/FileStore/tables/...\")\n",
    ">```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e89137bd-04b0-434b-9f1d-bd1afae10aa4",
     "showTitle": false,
     "title": ""
    },
    "id": "zfcEsiyTu9Q1"
   },
   "source": [
    "<a name=\"3\"></a>\n",
    "### 3. Vorbereitung der Trainings-, Validierungs- und Testdaten\n",
    "Um den Datensatz verwenden zu können müssen die Datenpunkte vorbereitet werden. Hierzu zählt das Bereinigen der Daten und das [Feature-Engineering](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114). Aus den Datensatz sollen so viele Informationen wie möglich extrahiert werden. Die Extrahierung zielt vorallem darauf ab die **Informationen zu erlangen**, **die nicht** explizipt **im Datensatz liegen**.<br>\n",
    "In unserem Fall versuchen wir aus einem großen Datensatz an Texten Informationen zu erlangen, die auf semantische Beziehungen schließen lassen, um eine Klassifikation nach Sarkasmus zu realisieren.<br><br>\n",
    "**Enscheidung:** Um einen generalisierten Anastz zu verfolgen, haben wir uns im Vorweg dagegen entschieden die weiteren Variablen des Datensatzes (bspw. *score*, *ups* und *downs*) in unser Modell einzubeziehen. Möglicherweise hätten diese uns zwar eine genauere Klassifikation ermöglicht, jedoch sind diese Informationen für die spätere generelle Anwendung nicht verfügbar (Eingrenzung des Use-Case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9ea62e63-07db-45c6-b2a1-6910778ea35a",
     "showTitle": false,
     "title": ""
    },
    "id": "-0sGm0Qqc6So"
   },
   "source": [
    "**Initialies Laden des Datensatzes:**<br>\n",
    "*Datensatz wird mittels der Pandas Libary aus der gegebenen CSV-Datei in ein Pandas DataFrame geladen. Für das lokale Ausführen kann hier der komplette Datensatz eingelesen werden. In Google Colab oder Databricks sollten kleinere Mengen gewählt werden.*<br>\n",
    ">Sollte bereits die Sub-Menge des Datensatzes generiert worden sein, so kann zu <a>Schritt 3.3</a> vorgesprungen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7aa69038-da4b-463a-a534-92fe8f88fae7",
     "showTitle": false,
     "title": ""
    },
    "id": "6sssdGV7cZGB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f3/2fx5j9s944361d_j_4t6jmh40000gn/T/ipykernel_23755/375597757.py:11: DtypeWarning: Columns (0,4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  complete_dataset = pd.read_csv(DATASET_PATH, sep=',', names=HEADER_LIST, encoding=\"ISO-8859-1\", nrows=READ_ROWS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300000 entries, 0 to 299999\n",
      "Data columns (total 10 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   label      300000 non-null  object\n",
      " 1   comment    299994 non-null  object\n",
      " 2   author     300000 non-null  object\n",
      " 3   subreddit  300000 non-null  object\n",
      " 4   score      300000 non-null  object\n",
      " 5   ups        300000 non-null  object\n",
      " 6   downs      300000 non-null  object\n",
      " 7   date       300000 non-null  object\n",
      " 8   ts         300000 non-null  object\n",
      " 9   parent     300000 non-null  object\n",
      "dtypes: object(10)\n",
      "memory usage: 22.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import pyspark\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_NAME = 'dataset.csv'\n",
    "DATASET_PATH = './' + DATASET_NAME\n",
    "\n",
    "READ_ROWS = 300000\n",
    "\n",
    "# For usage via Google Colab:\n",
    "HEADER_LIST = ['label', 'comment', 'author', 'subreddit', 'score', 'ups', 'downs', 'date', 'ts', 'parent']\n",
    "complete_dataset = pd.read_csv(DATASET_PATH, sep=',', names=HEADER_LIST, encoding=\"ISO-8859-1\", nrows=READ_ROWS)\n",
    "\n",
    "# For usage via Databricks:\n",
    "# complete_dataset = sqlContext.read.csv(DATASET_PATH, sep=\",\", header=True, encoding=\"ISO-8859-1\").limit(READ_ROWS)\n",
    "# complete_dataset = complete_dataset.toPandas()\n",
    "\n",
    "print(complete_dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b53a8ec5-f8f9-4496-8c26-6577b8196491",
     "showTitle": false,
     "title": ""
    },
    "id": "5bKPBqm3ZNQX"
   },
   "source": [
    "<a name=\"3.1\"></a>\n",
    "#### 3.1. Bereinigen der Daten\n",
    "Bevor jegliche Art der semeantischen Analyse oder dem Extrahieren einer relevanten und repräsentativen Submenge, müssen die Daten bereinigt werden. Es existieren mehrere Fehlerquellen in den Daten, die ohne eine Filtrierung die Egebnisse verfälschen würden.<br>\n",
    "Eine der vielen Fehlerquellen sind None-, NaN-, und leere Werte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3f30964a-a202-4d94-8a3d-0ab220c32fa9",
     "showTitle": false,
     "title": ""
    },
    "id": "mVTLYEIMc2oT"
   },
   "source": [
    "**Analysieren der Daten auf None-, NaN- und leere Werte:**<br>\n",
    "*Gibt die Summe aller None-, NaN- und leeren Werte aus.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3489fdec-45ce-454e-8473-8fbd89ad1d7a",
     "showTitle": false,
     "title": ""
    },
    "id": "a2YzYJwiczUw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summe Na-Werte: 6\n",
      "Summe leere Werte: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sum_na = complete_dataset.isna().sum().sum()\n",
    "sum_empty_string = complete_dataset.eq('').sum().sum()\n",
    "\n",
    "print(\"Summe Na-Werte: \" + str(sum_na))\n",
    "print(\"Summe leere Werte: \" + str(sum_empty_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bb034e9f-b671-4cd5-8bda-b0c0137bdd4a",
     "showTitle": false,
     "title": ""
    },
    "id": "TSyFULU2m0Bg"
   },
   "source": [
    "**Bereinigen der Daten nach None-, NaN- und leeren Werte:**<br>\n",
    "*Entfernt alle None-, NaN- und leeren Werte.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2a6fe308-a2f2-4e06-84b7-9f13d05c4cbc",
     "showTitle": false,
     "title": ""
    },
    "id": "ec9nQ1wqm-JP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summe Na-Werte: 0\n"
     ]
    }
   ],
   "source": [
    "complete_dataset = complete_dataset.dropna()\n",
    "\n",
    "sum_na = complete_dataset.isna().sum().sum()\n",
    "print(\"Summe Na-Werte: \" + str(sum_na))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bc9f1780-784f-4bff-b223-16a9d1600d58",
     "showTitle": false,
     "title": ""
    },
    "id": "0b-lYE10dDYy"
   },
   "source": [
    "**Entfernen von nicht benötigten Spalten:**<br>\n",
    "*Entfernt alle Spalten bis auf die Kommentare und Label.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "241c9118-b2b5-4f90-9ddb-f22621accaec",
     "showTitle": false,
     "title": ""
    },
    "id": "VwDWrXtxc0rd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['label', 'comment']\n",
      "Size: 299994\n"
     ]
    }
   ],
   "source": [
    "complete_dataset = complete_dataset[['label', 'comment']]\n",
    "\n",
    "print(\"Columns: \" + str(complete_dataset.columns.to_list()))\n",
    "print(\"Size: \" + str(len(complete_dataset.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGfs6n4bC7Cq"
   },
   "source": [
    "**Entfernen von nicht betrachteten Zeichen:**<br>\n",
    "*Entfernt aller Zeichen, die nicht für die Analyse benötigt werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5MbPLjuwC7Cq"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_empty(text):\n",
    "    if len(text.split()) == 0:\n",
    "        return ''\n",
    "    return text\n",
    "\n",
    "complete_dataset['comment'] = complete_dataset['comment'].apply(lambda text: re.sub('[^\\.\\?\\!\\,\\;\\sA-Za-z0-9]+', '', text))\n",
    "complete_dataset['comment'] = complete_dataset['comment'].apply(lambda text: replace_empty(text))\n",
    "complete_dataset['comment'] = complete_dataset['comment'].replace('', np.nan)\n",
    "complete_dataset = complete_dataset.dropna(subset=['comment'])\n",
    "complete_dataset = complete_dataset.drop_duplicates(subset=['comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c00d59ee-fe14-4bbe-9d39-00450526a52f",
     "showTitle": false,
     "title": ""
    },
    "id": "wt24e-tnoXGL"
   },
   "source": [
    "<a name=\"3.2\"></a>\n",
    "#### 3.2. Extrahieren einer ausgeglichenen Sub-Menge\n",
    "Da der Datensatz enorm viele Daten enthält, wäre er für die weitere Verarbeitung und Aufbereitung insofern ungeeignet, dass er die Rechenleistung enorm beeinträchtigt. Zudem steigt die Wahrscheinlichkeit des Overfittings bei steigender Datenpunktanzahl. Außerdem ist es wichtig, in diesem Schritt Ausreißer zu eleminieren.\n",
    "\n",
    "**Enscheidung:** Wir haben uns für eine gesamte Datenmenge von ` 6.000 Datenpunkten ` entschieden. Die sich wiederum später bei einem ` Train-, Test-Split auf 4.500, 1.500 Datenpunkten `."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2eba6f1a-b218-4504-be6b-e71d91b7cdd1",
     "showTitle": false,
     "title": ""
    },
    "id": "8O-dIDLpC7Cq"
   },
   "source": [
    "**Analysiert, ob ein Kommentar Zahlen enthält:**<br>\n",
    "*Durchläuft alle Zeichen der Zeichenfolge, um zu analysieren, ob Zahlen enthalten sind. Sollten diese enthalten sein, werden die Zeilen entfernt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1f74b16e-a86a-4260-99fa-c9d5732be3c2",
     "showTitle": false,
     "title": ""
    },
    "id": "vJJrw75TC7Cq"
   },
   "outputs": [],
   "source": [
    "def contains_digits(comment):\n",
    "    return any(chr.isdigit() for chr in comment)\n",
    "\n",
    "contains_digits_mask = np.vectorize(contains_digits)\n",
    "array_contains_digits = contains_digits_mask(complete_dataset['comment'].values.astype('U'))\n",
    "complete_dataset = complete_dataset[~array_contains_digits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "49d92714-1cf1-4c23-9526-3562a5e6686a",
     "showTitle": false,
     "title": ""
    },
    "id": "q0KrtQMXpOI9"
   },
   "source": [
    "**Analysieren der Häufigkeit von Wörtern:**<br>\n",
    "*Berechnet die Summer der Häufigkeit aller Wörter der Datenpunkte.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4e41f9da-03fb-4060-859e-aaf5459cdfd8",
     "showTitle": false,
     "title": ""
    },
    "id": "GWvyTKnfdGxE"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "word_counter = CountVectorizer(lowercase=True, analyzer='word', stop_words='english')\n",
    "array_word_counter = word_counter.fit_transform(complete_dataset['comment'].values.astype('U'))\n",
    "array_word_counter = np.sum(array_word_counter, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1159b2bc-ee93-4143-9f5d-2f218c168731",
     "showTitle": false,
     "title": ""
    },
    "id": "1C9ONxkyC7Cr"
   },
   "source": [
    "**Analysieren der Anzahl an Wörtern:**<br>\n",
    "*Berechnet die Anzahl an Wörtern der Datenpunkte.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1e2fc6c6-186f-4e94-9d1e-054b84a6c070",
     "showTitle": false,
     "title": ""
    },
    "id": "3CQxHDafC7Cr"
   },
   "outputs": [],
   "source": [
    "def count_words(comment):\n",
    "    return len(comment.split())\n",
    "\n",
    "comment_size_mask = np.vectorize(count_words)\n",
    "array_comment_size = comment_size_mask(complete_dataset['comment'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOXHFCbGC7Cr"
   },
   "source": [
    "**Analysieren der durchschnittlichen Wortgröße:**<br>\n",
    "*Berechnet die durchschnittliche Wortgröße der Datenpunkte.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "V1v_PtcyC7Cr"
   },
   "outputs": [],
   "source": [
    "def measure_words(comment):\n",
    "    words = comment.split()\n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "word_size_mask = np.vectorize(measure_words)\n",
    "array_word_size = word_size_mask(complete_dataset['comment'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8fbb9fe2-9230-4a71-8a2b-0250ed1012ba",
     "showTitle": false,
     "title": ""
    },
    "id": "Lv5d36_2C7Cr"
   },
   "source": [
    "**Konkatenieren der gesammelten Daten:**<br>\n",
    "*Zusammenführen der Daten und in einen Pandas Dataframe überführen. Die Anzahl und Häufigkeit der Wörter werden an die Datenpunkte angehängt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dbc9e152-1aea-4cc1-a39d-b96c98cebb05",
     "showTitle": false,
     "title": ""
    },
    "id": "yjVNZOXjC7Cs"
   },
   "outputs": [],
   "source": [
    "concatenated_data = np.array(np.hstack((\n",
    "        np.reshape(complete_dataset['label'].values, (-1, 1)),\n",
    "        np.reshape(complete_dataset['comment'].values, (-1, 1)),\n",
    "        np.reshape(array_comment_size, (-1, 1)),\n",
    "        np.reshape(array_word_counter, (-1, 1)),\n",
    "        np.reshape(array_word_size, (-1, 1)),\n",
    ")))\n",
    "concatenated_data = pd.DataFrame(concatenated_data, columns=['label', 'comment', 'comment_size', 'word_count', 'avg_word_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad019dac-5fe8-407c-ae43-ce2386298921",
     "showTitle": false,
     "title": ""
    },
    "id": "4kbNHTBuC7Cs"
   },
   "source": [
    "**Filtern und Sortieren der Daten:**<br>\n",
    "*Die Daten werden im Datenframe nach der Anzahl an Wörtern gefiltert, wobei diese gegen eine Mindestwortanzahl gepfüft werden. Anschließend werden die Daten nach der Anzahl der Wörter und dessen Häufigkeit sortiert.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "680c2a20-0e6f-482a-bead-4f28d3f13ae4",
     "showTitle": false,
     "title": ""
    },
    "id": "cKbZVJ9hC7Cs"
   },
   "outputs": [],
   "source": [
    "MIN_WORD_COUNT = 4\n",
    "\n",
    "concatenated_data = concatenated_data[concatenated_data['comment_size'] >= MIN_WORD_COUNT]\n",
    "concatenated_data = concatenated_data.sort_values(by=['word_count', 'avg_word_size', 'comment_size'], ascending=[True, False, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "14bfee95-ddf9-428b-afdc-988b5bad7978",
     "showTitle": false,
     "title": ""
    },
    "id": "fqMxRF5XC7Cs"
   },
   "source": [
    "**Extrahieren eines gleich großen Submenge:**<br>\n",
    "*Aus dem Dataframe wird eine gleich große Submenge an positiven und negativen Sarkasmusbefunden extrahiert und konkateniert (`6.000 Datenpunkte`).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf7582bb-bbc7-4ca3-a193-5cf3641c951c",
     "showTitle": false,
     "title": ""
    },
    "id": "-_mMIiZZC7Cs"
   },
   "outputs": [],
   "source": [
    "SUBSET_PART_SIZE = 6000\n",
    "\n",
    "# Note: For Databricks please use char representation instead.\n",
    "POSITIVE_LABEL = 1\n",
    "NEGATIVE_LABEL = 0\n",
    "\n",
    "concatenated_data_positive = concatenated_data[concatenated_data['label'] == POSITIVE_LABEL].head(SUBSET_PART_SIZE)\n",
    "concatenated_data_negative = concatenated_data[concatenated_data['label'] == NEGATIVE_LABEL].head(SUBSET_PART_SIZE)\n",
    "concatenated_data = pd.concat([concatenated_data_positive, concatenated_data_negative])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f90d60ee-e7cc-4735-9026-1149a82452bf",
     "showTitle": false,
     "title": ""
    },
    "id": "ZeiGBG_qC7Ct"
   },
   "source": [
    "**Abspeichern der neuen Submenge:**<br>\n",
    "*Das Dataframe wird in eine CSV-Datei abgespeichert, um diese im späteren Verlauf zu laden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2cc5db43-8d4b-4d4a-8f71-8cc33c0ef7aa",
     "showTitle": false,
     "title": ""
    },
    "id": "swqOHbQdC7Ct"
   },
   "outputs": [],
   "source": [
    "SUBSET_NAME = 'subset-dataset.csv'\n",
    "OUTPUT_PATH = './' + SUBSET_NAME\n",
    "\n",
    "concatenated_data = concatenated_data.loc[:, ~concatenated_data.columns.isin(['word_count', 'comment_size', 'avg_word_size'])]\n",
    "\n",
    "# For usage via Google Colab:\n",
    "concatenated_data.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "# For usage via Databricks:\n",
    "# sparkDataFrame = spark.createDataFrame(concatenated_data)\n",
    "# sparkDataFrame.coalesce(1).write.csv(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9a11d481-f3fd-40aa-a184-a743c0a88b6e",
     "showTitle": false,
     "title": ""
    },
    "id": "QGGYCmL6C7Ct"
   },
   "source": [
    "---\n",
    "> ### Information:\n",
    "> Die nachfolgende Schritte sollten erst ausgeführt werden, wenn die Submenge in <a>Schritt 3.2</a> bereits in eine CSV-Datei gespeichert wurde. Diese CSV-Datei soll im folgenden als neue Datenquelle fungieren und muss dementsprechend erst eingelesen werden Sollte das Vocabular auch bereits vorliegen, so kann zu <a>Schritt 3.5</a> gesprungen werden.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9e227689-6c53-458b-ab45-727b28de7190",
     "showTitle": false,
     "title": ""
    },
    "id": "khZ4COJXC7Ct"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12000 entries, 0 to 11999\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   label    12000 non-null  int64 \n",
      " 1   comment  12000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 187.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import pyspark\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_NAME = 'subset-dataset.csv'\n",
    "DATASET_PATH = './' + DATASET_NAME\n",
    "\n",
    "# For usage via Google Colab:\n",
    "HEADER_LIST = ['label', 'comment']\n",
    "complete_dataset = pd.read_csv(DATASET_PATH, names=HEADER_LIST, header=0)\n",
    "\n",
    "# For usage via Databricks:\n",
    "# complete_dataset = sqlContext.read.csv(DATASET_PATH, header=False).toDF('label', 'comment')\n",
    "# complete_dataset = complete_dataset.toPandas()\n",
    "\n",
    "print(complete_dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "37a83d3c-de17-4752-98dd-b52935809ff8",
     "showTitle": false,
     "title": ""
    },
    "id": "gLs7aW7hC7Ct"
   },
   "source": [
    "<a name=\"3.3\"></a>\n",
    "#### 3.3. Lemmatizing der Texte\n",
    "Das <a>Lemmmatizing</a> setzt alle Wörter auf ihren Wortstamm und verringert somit die Anzahl an einzigartigen Werten zwischen den Texten. Dies ist vorallem wichtig, um im späteren Verlauf die Wörterhäufigkeit im gesamten Dokument einheitlich bemessen zu können. Außerdem kann anhand der verkürzten Wörter ein wesentlich effizienteres Vokabular aufgebaut werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3842f11a-e3c2-4b29-8be6-3cf629d132ef",
     "showTitle": false,
     "title": ""
    },
    "id": "tO2rhkhtC7Ct"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/fprinz/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/fprinz/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/fprinz/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/fprinz/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def lemmatize_words(comment):\n",
    "    word_list = pos_tag(tokenizer.tokenize(comment))\n",
    "    lemmatize_words = []\n",
    "    for word, tag in word_list:\n",
    "        wtag = tag[0].lower()\n",
    "        if wtag in ['a', 'r', 'n', 'v', 's']:\n",
    "            word = lemmatizer.lemmatize(word, wtag)\n",
    "        lemmatize_words.append(word)\n",
    "    return ' '.join(lemmatize_words)\n",
    "\n",
    "lemmatize_words_mask = np.vectorize(lemmatize_words)\n",
    "complete_dataset['comment'] = lemmatize_words_mask(complete_dataset['comment'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a9ca3488-0ebd-4777-bcd9-72e28ac4c436",
     "showTitle": false,
     "title": ""
    },
    "id": "_-wddXg8C7Cu"
   },
   "source": [
    "**Was ist ein Tokenizer?**<br>\n",
    "Ein Tokenizer dient zur Auftrennung von Zeichenfolgen basierend auf festgelegten Regeln. Je nach Implementierung kann der Tokenizer rudimentäre Wortzerlegung durchführen oder basierend auf der jeweiligen Sprache spezifische Worttrennungen durchführen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "83f662b8-d108-448e-9c62-7acd08daaa4e",
     "showTitle": false,
     "title": ""
    },
    "id": "6FMgb1A2C7Cu"
   },
   "source": [
    "**Was sind Pos-Tags?**<br>\n",
    "Die Pos-Tags geben an, um welche Art von Wort es sich im Kontext des Satzes handelt. Unterschieden wird beispielsweise nach *Nomen*, *Verben* oder *Subjekten*. Somit kann der Lemmatizer die Wortstämme besser identifizieren und die Anzahl an Fehlbildungen geht zurück."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "60357663-1215-4c25-928d-4163c99c76ef",
     "showTitle": false,
     "title": ""
    },
    "id": "AsRcgzo4C7Cu"
   },
   "source": [
    "**Was ist der Unterschied zwischen Stemming und Lemmatizing?**<br>\n",
    "*Das Stemming schneidet Konjugationen einzelner Wörter ab und verringert somit die Anzahl an einzigartigen Werten zwischen den Texten. Da hierbei allerdings nicht immer der Wortstamm zurückgegeben wird, leidet die Vergleichbarkeit der Texte. Da das Stemming im Vergleich zum Lemmatizing lediglich die Konjugation abschneidet und das Wort nicht auf den Wortstamm zurück bildet, kann man sagen, dass das Stemming vergleichsweise rudimentär funktioniert.*<br>\n",
    "```\n",
    "stemmer = PorterStemmer()\n",
    "word_stemmed = stemmer.stem(\"studies\")\n",
    "> studi\n",
    "```\n",
    "```\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "word_lemmatized = lemmatizer.lemmatize(\"studies\")\n",
    "> study\n",
    "```\n",
    "<br>**Entscheidung:** Da es uns um die größt mögliche semantische Korrektheit bei hoher Effizienz geht, haben wir uns für das Lemmatizing entschieden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "26da9681-19f2-4702-a701-6688060a00a9",
     "showTitle": false,
     "title": ""
    },
    "id": "GrMs02wQC7Cu"
   },
   "source": [
    "**Entfernen der Punktuationen:**<br>\n",
    "*Für die Lemmatization ist es wichtig die Satztrennzeichen in den Kommentaren bei zu behalten. Diese können nun entfernt werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a202571e-5e30-4f5e-87de-e0628386ec5a",
     "showTitle": false,
     "title": ""
    },
    "id": "bQ-vcRX5C7Cu"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    if type(text) != str or len(text) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    clean_text = \"\"\n",
    "    for char in text:\n",
    "        if char not in string.punctuation:\n",
    "            clean_text += char\n",
    "    return clean_text\n",
    "\n",
    "complete_dataset['comment'] = complete_dataset['comment'].apply(lambda text: remove_punctuation(text))\n",
    "complete_dataset = complete_dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "09e11fec-67da-4ca6-a4db-9466822977f3",
     "showTitle": false,
     "title": ""
    },
    "id": "NWphm1nFC7Cu"
   },
   "source": [
    "**Abspeichern der neuen Datenmenge:**<br>\n",
    "*Das Dataframe wird in eine CSV-Datei abgespeichert, um diese im späteren Verlauf zu laden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5b006cb9-dd91-4c2c-a45a-fd37282fa4dd",
     "showTitle": false,
     "title": ""
    },
    "id": "3taepByGC7Cu"
   },
   "outputs": [],
   "source": [
    "SUBSET_NAME = 'prepared-dataset.csv'\n",
    "OUTPUT_PATH = './' + SUBSET_NAME\n",
    "\n",
    "# For usage via Google Colab:\n",
    "complete_dataset.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "# For usage via Databricks:\n",
    "# sparkDataFrame = spark.createDataFrame(complete_dataset)\n",
    "# sparkDataFrame.coalesce(1).write.csv(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6d0ee927-72c4-4d6e-904a-c9bf41f8c024",
     "showTitle": false,
     "title": ""
    },
    "id": "80WvzeN1C7Cu"
   },
   "source": [
    "<a name=\"3.4\"></a>\n",
    "#### 3.4. Erstellen eines Bag-Of-Words Vocabulars\n",
    "In diesem Schritt soll die Submenge der Datenbasis in ein <a>Vocabular</a> aufgebrochen werden, welche sich aus den <a>Bag-Of-Words</a> ergeben.\n",
    "Dafür benutzen wir einen <a>CountVectorizer</a> und extrahieren die trainierten Features, die aus sogenannten <a>N-Grams</a> bestehen. Diese können wir dann als Array in eine CSV-Datei abspeichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df1c038a-6f9c-4932-9642-e427907f2ecf",
     "showTitle": false,
     "title": ""
    },
    "id": "pL2WatnbC7Cv"
   },
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "VOCABULARY_NAME = 'vocabulary.csv'\n",
    "OUTPUT_PATH = './' + VOCABULARY_NAME\n",
    "NGRAM_RANGE = (1,3)\n",
    "\n",
    "count_vectorizer = CountVectorizer(lowercase=True, analyzer='word', stop_words='english', ngram_range=NGRAM_RANGE)\n",
    "count_vectorizer.fit_transform(complete_dataset['comment'].values.astype('U'))\n",
    "\n",
    "# Note: For newer versions of scikit-learn use get_feature_names_out().\n",
    "\n",
    "# For usage via Google Colab:\n",
    "df_vocabulary = pd.DataFrame(count_vectorizer.get_feature_names_out())\n",
    "df_vocabulary.to_csv(OUTPUT_PATH, index=False, header=False)\n",
    "\n",
    "# For usage via Databricks:\n",
    "# df_vocabulary = pd.DataFrame(count_vectorizer.get_feature_names())\n",
    "# sparkDataFrame = spark.createDataFrame(df_vocabulary)\n",
    "# sparkDataFrame.coalesce(1).write.csv(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2cb34c8f-3d83-468e-ac18-253e56748ddb",
     "showTitle": false,
     "title": ""
    },
    "id": "MfjIryZ7C7Cv"
   },
   "source": [
    "**Was ist Bag-Of-Words?**<br>\n",
    "Die Bag-of-Words Methode dient im Bereich des Language Processing der Erzeugung nummerischer Features aus einer gegebenen Menge an textuellen Eingaben. Dazu wird zunächst ein Vokabular aus der gegebenen Menge an Eingabetexten erzeugt (siehe unten).\n",
    "Die Bag-of-Words Repräsentation eines Satzes gibt an, aus welchen Wörtern sich der Satz zusammensetzt. Sie gibt dabei allerdings keine verlässliche Auskunft darüber, wie diese angeordnet sind. Vielmehr besteht sie aus einem Vektor mit der Dimension n (n=Länge des Vokabulars). Jedes einzelne Merkmal des Vektors gibt dabei an, ob das durch das Merkmal repräsentierte Wort des Vokabulars teil dies Satzes ist.\n",
    "\n",
    "<u>Beispiel:</u>\n",
    "\n",
    "Satz 1: Das ist toll.\n",
    "\n",
    "Satz 2: Das ist schlecht.\n",
    "\n",
    "Vokabular (Ohne Interpunktion und Großschreibung)= [das; ist; toll; schlecht]\n",
    "\n",
    "B-o-W 1: (1; 1; 1; 0)\n",
    "\n",
    "B-o-W 1: (1; 1; 0; 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "757ffbb8-2ef3-40e2-bc65-66951431185f",
     "showTitle": false,
     "title": ""
    },
    "id": "8msk5cXpC7Cv"
   },
   "source": [
    "**Was sind N-Grams?**<br>\n",
    "N-Grams sind eine Möglichkeit den Zusammenhang zwischen verschiedenen Worten zu analysieren. Dies geschieht durch eine gemeinsame Erfassung verschiedener Textabschnitte, deren Länge vom Anwender definiert werden kann.\n",
    "\n",
    "<u>Beispiel</u><br>\n",
    "\"Dieser Satz dient als Beispiel für NGrams.\"<br>\n",
    "\n",
    "NGrams (Ohne Interpunktion und Großschreibung)<br>\n",
    "\n",
    "Monograms: [\"dieser\"; \"satz\"; \"dient\"; \"als\"; \"beispiel\"; \"für\"; \"ngrams\"]<br>\n",
    "Bigrams: [\"dieser satz\"; \"satz dient\"; \"dient als\"; \"als beispiel\"; \"beispiel für\"; \"für ngrams\"]<br>\n",
    "Trigrams: [\"dieser satz dient\"; \"satz dient als\"; \"dient als beispiel\"; \"als beispiel für\"; \"beispiel für ngrams\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "29610614-9faa-42cb-915c-d8674fa52548",
     "showTitle": false,
     "title": ""
    },
    "id": "bUC-Dp--C7Cv"
   },
   "source": [
    "**Was ist ein Vokabular?**<br>\n",
    "Ein Vokabular bezeichnet die Zuordnung von Texttermen auf die Indizes eines numerischen Featurevektors. Sollte ein Vokabular nicht bei der Instanziierung übergeben werden, ist es den FeatureExtraktoren von SKlearn möglich, basierend auf den Trainingsdaten eines zu erzeugen.<br>\n",
    "\n",
    "<u>Beispiel:</u><br>\n",
    "\"Dieser Satz dient als Beispiel für ein Vokabular.\"<br>\n",
    "Vokabular (ohne Interpunktion und Großschreibung): {\"dieser\": 0; \"satz\": 1; \"dient\": 2; \"als\": 3; \"beispiel\": 4; \"für\": 5; \"ein\": 6; \"vokabular\": 7}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d54922c9-c835-4ce6-9f80-c70b53fb96e8",
     "showTitle": false,
     "title": ""
    },
    "id": "pkaqDlh9C7Cv"
   },
   "source": [
    "**Was ist ein CountVectorizer?**<br>\n",
    "Der von SKlearn zur Verfügung gestellte CountVectorizer stellt eine Möglichkeit dar, aus Texten in String-Repräsentation numerische Vektoren zu generieren.\n",
    "Hierzu lernt der Vecotrizer zunächst eine gegebene Menge an Texten um ein Vokabular zu erzeugen, auf dessen Basis er im Folgenden die Transformation der gelieferten Texte in Vektoren vornehmen kann. Alternativ kann dieses Vokabular über den *vocabulary*-Parameter an den Vectorizer übergeben werden.\n",
    "Sobald der *fitting*-Vorgang beendet ist, ist der Vectorizer in der Lage Texte zu konvertieren. Das Ergebnis ist ein Vektor der Dimension n (n=Länge des gelernten Vokabulars) dessen einzelne Merkmale jeweils die Anzahl an Auftritten des durch den Index i repräsentierten Wertes des Vokabulars im Text darstellen.<br>\n",
    "```\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "```\n",
    "Der erzeugte Vectorizer wurde im obigen Beispiel dahingehend parametrisiert, dass er alle gegeben Texteingaben zu Kleinbuchstaben konvertiert sowie die englischen Stopwords entfernt. Bei sogenannten Stopwords handelt es sich um Wörter, die in einer gegebenen Sprachen (hier Englisch) keine Bedeutung innerhalb eines Satzes hinzufügen. Dazu gehören unter anderem 'the', 'it', 'am' etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6bb533b6-5f4f-408e-83c7-afb6d9e9d96e",
     "showTitle": false,
     "title": ""
    },
    "id": "w6ggd3XvC7Cv"
   },
   "source": [
    "---\n",
    "> ### Information:\n",
    "> Die nachfolgende Schritte sollten erst ausgeführt werden, wenn die Submenge in <a>Schritt 3.2</a> und das Vocabular in <a>Schritt 3.4</a> bereits in eine CSV-Datei gespeichert wurde. Diese CSV-Dateien soll im folgenden als neue Datenquelle fungieren und müssen dementsprechend erst eingelesen werden.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba05cbd9-3b0c-4b34-aeeb-ace6f59a5389",
     "showTitle": false,
     "title": ""
    },
    "id": "To8pvk8jC7Cv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12000 entries, 0 to 11999\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   label    12000 non-null  int64 \n",
      " 1   comment  12000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 187.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13049 entries, 0 to 13048\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       13049 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 102.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import pyspark\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_NAME = 'prepared-dataset.csv'\n",
    "DATASET_PATH = './' + DATASET_NAME\n",
    "VOCABULARY_NAME = 'vocabulary.csv'\n",
    "VOCABULARY_PATH = './' + VOCABULARY_NAME\n",
    "\n",
    "# For usage via Google Colab:\n",
    "HEADER_LIST = ['label', 'comment']\n",
    "complete_dataset = pd.read_csv(DATASET_PATH, names=HEADER_LIST, header=0)\n",
    "vocabulary = pd.read_csv(VOCABULARY_PATH, header=None)\n",
    "\n",
    "# For usage via Databricks:\n",
    "# complete_dataset = sqlContext.read.csv(DATASET_PATH, header=False).toDF('label', 'comment')\n",
    "# complete_dataset = complete_dataset.toPandas()\n",
    "# vocabulary = sqlContext.read.csv(VOCABULARY_PATH, header=False)\n",
    "# vocabulary = vocabulary.toPandas()\n",
    "\n",
    "print(complete_dataset.info())\n",
    "print(vocabulary.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "01e60dc9-a7bd-495a-9790-494f78753b3d",
     "showTitle": false,
     "title": ""
    },
    "id": "kcqiTG7XC7Cw"
   },
   "source": [
    "<a name=\"3.5\"></a>\n",
    "#### 3.5. Generieren der Trainings- und Testdaten\n",
    "In diesem Schritt werden die Trainings- und Testdaten erst komplett generiert und im Anschluss mit Hilfe der `train_test_split` Methode aufgeteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "856e08eb-e47b-4f2b-886e-0fd6ca52d241",
     "showTitle": false,
     "title": ""
    },
    "id": "3e2Vk4dpC7Cw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Note: Please choose the same NGRAM_RANGE as in the vocabulary.\n",
    "NGRAM_RANGE = (1,3)\n",
    "\n",
    "count_vectorizer = CountVectorizer(lowercase=True, analyzer='word', stop_words='english', ngram_range=NGRAM_RANGE, vocabulary=vocabulary.iloc[:, 0].values.astype('U'))\n",
    "bag_of_words = count_vectorizer.fit_transform(complete_dataset['comment'].values.astype('U'))\n",
    "\n",
    "complete_x = np.array(bag_of_words.todense())\n",
    "complete_y = complete_dataset['label'].values.astype('int32')\n",
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(complete_x, complete_y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "892fca03-897b-4ed9-9d49-97e479e13716",
     "showTitle": false,
     "title": ""
    },
    "id": "eOY0y846C7Cw"
   },
   "source": [
    "<a name=\"4\"></a>\n",
    "### 4. Initialisieren und Trainieren des Modells\n",
    "In diesem Schritt wird die Kernel Ridge Regression erstmalig im Rahmen eines Modells initialisiert. Dieses Modell wird im Anschluss trainiert und durch <a>Hyperparametertuning</a> optimiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d427dfd6-7908-4a6c-8800-b80a2388f5ef",
     "showTitle": false,
     "title": ""
    },
    "id": "I7MbiNMwC7Cw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, estimator=KernelRidge(), n_jobs=2,\n",
       "             param_grid={'alpha': [1, 2, 0.95], 'coef0': [0.95],\n",
       "                         'degree': [2, 1], 'gamma': [0.1, 0.2],\n",
       "                         'kernel': ['poly']},\n",
       "             scoring=make_scorer(score), verbose=4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "def score(y_true, y_pred):\n",
    "    y_pred = np.rint(y_pred)\n",
    "    return f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "scorer = make_scorer(score)\n",
    "\n",
    "model = GridSearchCV(\n",
    "    estimator=KernelRidge(),\n",
    "    param_grid={\n",
    "        'alpha': [1, 2, 0.95], \n",
    "        'degree': [2, 1], \n",
    "        'gamma': [0.1, 0.2], \n",
    "        'coef0': [0.95], \n",
    "        'kernel': ['poly']\n",
    "    },\n",
    "    verbose=4,\n",
    "    scoring=scorer,\n",
    "    n_jobs=2,\n",
    "    cv=2\n",
    ")\n",
    "model.fit(x_train, np.asarray(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6w0cU_FgC7Cw"
   },
   "source": [
    "Der `GridSearchCV` ist eine Klasse zur generischen Optimierung einer Machine Learning Methode anhand von definierten Hyperparametern. Über den Konstruktorparameter `param_grid` kann ein Dictionary an Paramtern für den Estimator übergeben werden. Im Dictionary stehen die Möglichkeiten der einzelnen Paramter, die auf die beste Kombination unterscuht werden sollen. Außerdem können unterschiedliche Kernel angegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "fgAZFVmRC7Cw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter: {'alpha': 0.95, 'coef0': 0.95, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "Erzielter Tuning-Score: 0.5838888888888889\n"
     ]
    }
   ],
   "source": [
    "print('Beste Parameter: ' + str(model.best_params_))\n",
    "print('Erzielter Tuning-Score: ' + str(model.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXj2gPcEC7Cw"
   },
   "source": [
    "Aus dem `GridSearchCV` können die besten Parameter und der beste Score dieser Parameter im Tuning ausgegeben werden. Außerdem kann der beste Estimator auf neuen Daten angewandt werden, um bspw. einen Score zu berechnen. Hierbei wird einheitlich auf die im `GridSearchCV` definierte Score-Funktion verwiesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9d81a3b2-e971-4c33-8550-9eddfdcd888f",
     "showTitle": false,
     "title": ""
    },
    "id": "h2MojyXBC7Cw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Score: 0.8877777777777778\n",
      "Test-Score: 0.607\n"
     ]
    }
   ],
   "source": [
    "print(\"Train-Score: \" + str(model.score(x_train, y_train)))\n",
    "print(\"Test-Score: \" + str(model.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "55e8f08f-08e5-48ba-9b25-65789eaa1e5d",
     "showTitle": false,
     "title": ""
    },
    "id": "Xhb8zX7FC7Cx"
   },
   "source": [
    "**Was ist ein Hyperparametertuning?**<br>\n",
    "Als Hyperparametertuning wird die iterative Anpassung der Parameter bezeichnet, die vor dem Trainingsvorgang eines Modells festgelegt werden können. Ihre Wahl kann für die Güte eines Modells von entscheidender Wichtigkeit sein, weswegen ihre Optmimierung ein wichtiger Schritt bei der Modellentwicklung ist.<br>\n",
    "Während des Optimierungsvorgangs wird das Modell mehrmals mit unterschiedlichen Hyperparametern trainiert um anschließend seine Genauigkeit im Hinblick auf Trainings- und Testdaten zu bestimmen. Für jedes Set an Hyperparametern (im folgenden als Konfiguration bezeichnet) speichert sich das Modell die Güteindikatoren um nach Ablauf der Iterationen bestimmen zu können, welche Konfiguration das beste Ergebnis hervorbrachte. Die am besten bewertete Konfiguration wird dann als Resultat des Tunings bereitgestellt.<br><br>\n",
    "Im obigen Beispiel wird das GridSearch Verfahren genutzt. Hierbei ist für die zu optimierenden Parameter eine Vorauswahl getroffen worden, aus der sich das GridSearch Verfahren Konfigurationen zusammenstellen kann. Somit werden verschiedene Variationen von Hyperparametern überprüft um die beste Konfiguration bestimmen zu können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0c8ec7fc-a639-49b3-bb67-3be0a827664c",
     "showTitle": false,
     "title": ""
    },
    "id": "ItWkbM8kC7Cx"
   },
   "source": [
    "<a name=\"4.1\"></a>\n",
    "#### 4.1. Ergebnisse des Tunings\n",
    "|ID|N|n|NGram|Kernel|Alpha|Train|Test\n",
    "|--|-|-|-----|-----|------|-----|----\n",
    "|1|200.000|1.000|(1,10)|Poly|0.001|0.66|-0.08\n",
    "|2|50.000|1.000|(1,10)|Poly|0.001|0.75|-0.06\n",
    "|3|50.000|10.000|(1,10)|Poly|0.001|0.49|0.09\n",
    "|4|50.000|10.000|(2,3)|Poly|0.001|0.37|0.01\n",
    "|5|50.000|10.000|(1,20)|Poly|0.001|0.49|0.09\n",
    "|6|500|10|(1,10)|Poly|0.001|0.99|-1.47\n",
    "|7|100.000|10.000|(1,10)|Poly(d=2)|0.001|0.41|0.08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnQi1_OtC7Cx"
   },
   "source": [
    "**Umstellung der Scoring-Funktion:**<br>\n",
    "Nach einigen Trainingsdurchläufen ist aufgefallen, dass die Bewertung der ermittelten Werte fehlerhaft ist. Die verwendete Scoring-Funktion hat sich automatisch auf `accuracy` umgestellt. Bei dieser wird die Güte anhand der Genauigkeit ermittelt (Abweichung vom tatsächlichen Wert). Dies ist besonders in der Klassifikation unbrauchbar, da hierbei ein Score der die Klassifizierungsfehler/-quote enthält wesentlich genauer bewertet. Daher haben wir uns für den `f1_score` entschieden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcaFh3NLC7Cx"
   },
   "source": [
    "|ID|N|n|NGram|Kernel|Alpha|Train|Test\n",
    "|--|-|-|-----|-----|------|-----|----\n",
    "|1|50.000|10.000|(1,10)|Poly|0.001|0.87|0.55\n",
    "|2|100.000|20.000|(1,3)|Poly|0.001|0.81|0.59\n",
    "|3|200.000|5.000|(1,3)|Sigmoid(c0=1.1,g=0.4)|0.1|0.86|0.59\n",
    "|3|200.000|6.000|(1,3)|Poly(c0=0.95,g=0.1)|0.95|0.83|0.58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckcvg9GYC7Cx"
   },
   "source": [
    "**Weitere Erkenntnisse:**<br>\n",
    "Bei `scikit-learn` werden einige Regularien in Implementationen integriert, die bei ineffizienter Nutzung von Modellen greifen. Eine dieser Regularien betrifft die Ridge Regression, die an Effektivität für Inputdaten mit höherer Dimensionsanzahl als Trainingsdaten verliert. Hier wird automatisch auf eine andere Gewichtungs-/Optimierungsfunktionen umgestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "WB-oYjPrC7Cx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best-model.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Speichern des Modells und des Suchcontainers\n",
    "joblib.dump(model, 'grid-search_model.pkl')\n",
    "joblib.dump(model.best_estimator_, 'best-model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "91a984cc-10cc-4a84-931f-d0fb9d3d4e0b",
     "showTitle": false,
     "title": ""
    },
    "id": "HnEYCp0tC7Cx"
   },
   "source": [
    "<a name=\"4.2\"></a>\n",
    "#### 4.2. Exkurs: Falkon-ML\n",
    "Bei Falkon-ML handelt es sich um eine verbesserte Variante von Kernel Ridge Regression. Um Kernel Ridge Regression zu verbessern kann vorallem die Komplexität des Modells vereinfacht werden. Um dieses Ziel zu erreichen, werden die Kernel-Methoden approximiert. Diese Funktionalität wird vorallem durch das Konzept der Random Projections gegeben. Hierbei handelt es sich um Projektionen in einen Subspace eines Datenraums. Somit werden keine vordefinierten Kernel verwendet, sondern passende zufällige Kernel gewählt. Diese werden dann im Trainingsprozess optimiert. Diese reduzieren die Rechenzeit und erhöhen die Flexibilität hinsichtlich hochdimensionalen Daten.<br><br>\n",
    "**Zusätzliche Features:**<br>\n",
    "- Full multi-GPU support - All compute-intensive parts of the algorithms are multi-GPU capable.\n",
    "- Extreme scalability - Unlike other kernel solvers, we keep memory usage in check. We have tested the library with datasets of billions of points.\n",
    "- Sparse data support\n",
    "- Scikit-learn integration - Our estimators follow the scikit-learn API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3b60c0f-e7b8-42c6-aafa-37e240a33093",
     "showTitle": false,
     "title": ""
    },
    "id": "b27RhJ-sC7Cx"
   },
   "source": [
    "<a name=\"5\"></a>\n",
    "### 5. Nutzen des Modells\n",
    "Nachdem das Modell trainiert wurde, kann es für die Vorhersage/Klassifikation genutzt werden. Hierzu müssen alle benötigten Libaries erneut importiert werden. Außerdem muss die Funktionalität der Vorverarbeitung bereitgestellt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "bBjhRixrC7Cy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/fprinz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/fprinz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/fprinz/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/fprinz/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "VOCABULARY_NAME = 'vocabulary.csv'\n",
    "VOCABULARY_PATH = './' + VOCABULARY_NAME\n",
    "NGRAM_RANGE = (1,3)\n",
    "\n",
    "vocabulary = pd.read_csv(VOCABULARY_PATH, header=None)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sx9RwTFyC7Cy"
   },
   "source": [
    "Nachdem das Vokabular eingelesen und die Lemmatizer/Tokenizer initialisiert wurden, können die benötigten Funktionen erneut definiert werden. Nachdem die Daten abgespeichert wurden, verlieren die Referenzen die Gültigkeit. Die Referenzen zu Variablen und Methoden kann nur bestehen wenn alle benötigten Paramerter erneut definiert werden. Im folgenden Schritt wird die Score-Funktion erneut definiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dssTfOqSC7Cy"
   },
   "outputs": [],
   "source": [
    "def score(y_true, y_pred):\n",
    "    y_pred = np.rint(y_pred)\n",
    "    return f1_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWe1gEkqC7Cy"
   },
   "source": [
    "Zudem müssen die Vorbereitungsfunktionen zum Entfernen der Punktuationen und dem Lemmatisieren definiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "l_LXzdP1C7Cy"
   },
   "outputs": [],
   "source": [
    "def lemmatize_words(comment):\n",
    "    word_list = pos_tag(tokenizer.tokenize(comment))\n",
    "    lemmatize_words = []\n",
    "    for word, tag in word_list:\n",
    "        wtag = tag[0].lower()\n",
    "        if wtag in ['a', 'r', 'n', 'v', 's']:\n",
    "            word = lemmatizer.lemmatize(word, wtag)\n",
    "        lemmatize_words.append(word)\n",
    "    return ' '.join(lemmatize_words)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    if type(text) != str or len(text) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    clean_text = \"\"\n",
    "    for char in text:\n",
    "        if char not in string.punctuation:\n",
    "            clean_text += char\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rr3_yt_pC7Cy"
   },
   "source": [
    "Zusammengefasst werden die Vorbereitungsfunktionen in der `generate_features` Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "_THnO7WSC7Cy"
   },
   "outputs": [],
   "source": [
    "def generate_features(sentence):\n",
    "    sentence = lemmatize_words(sentence)\n",
    "    sentence = remove_punctuation(sentence)\n",
    "    count_vectorizer = CountVectorizer(lowercase=True, analyzer='word', stop_words='english', ngram_range=NGRAM_RANGE, vocabulary=vocabulary.iloc[:, 0].values.astype('U'))\n",
    "    bag_of_words = count_vectorizer.fit_transform([sentence]) \n",
    "\n",
    "    return np.array(bag_of_words.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxHjkYSsC7Cy"
   },
   "source": [
    "Letztlich kann das Modell auf einen manuell definierten Satz angewandt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "DcCymVyfC7Cz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ergebnis der Vorhersage für 'My name is Tjark!':\n",
      "Kein Sarkasmus [0.43498561]\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load('grid-search_model.pkl')\n",
    "\n",
    "SENTENCE = \"My name is Tjark!\"\n",
    "\n",
    "features = generate_features(SENTENCE)\n",
    "prediction = model.predict(features)\n",
    "prediction_rounded = np.rint(prediction)\n",
    "if (prediction_rounded[0] == 1):\n",
    "    predicted_class = \"Sarkasmus\"\n",
    "else:\n",
    "    predicted_class = \"Kein Sarkasmus\"\n",
    "\n",
    "print(\"Ergebnis der Vorhersage für '\" + str(SENTENCE) + \"':\\n\" + predicted_class + \" \" + str(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3ddee134-a7a8-47c8-8afd-e9ca30ffbc14",
     "showTitle": false,
     "title": ""
    },
    "id": "_2IHcCVAC7Cz"
   },
   "source": [
    "<a name=\"6\"></a>\n",
    "### 6. Einbindung in eine Web-Anwendung\n",
    "Zusätzlich zur Nutzung über den Code haben wir einen Web-Anwendung implementiert, die das Testen des Modells vereinfacht. Hierzu verwenden wir als Basis eine Express.js Applikation, die eine einfache HTML-Seite im JADE-Template-Format rendert. Innerhalb des Templates ist eine Methode definiert, die auf Knopfdruck einen Text, den der Nutzer eingegeben hat, an den Webserver sendet. Das senden findet klassisch über einen AJAX-Request statt. Das Backend startet im Anschluss einen sog. `child_process`. Hierbei handelt es sich um einen gekapselte Shell, die ein `.sh` Skript ausführt. Dieses übernimmt die Kommunikation mit dem virtuellen Environment von Python. Der `child_process` wartet über das Skript auf einen Antwort im JSON-Format von dem Python Skript. Das Python Skript lädt das Vokabular und das Modell (dieser Prozess sollte in Zukunft verbessert werden). Anschließend wird das Modell mit dem Text aus der Textbox ausgeführt und das Ergebnis zurückgegeben. Der `child_process` wird automatisch beendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df17b4aa-0859-4f1e-922f-148db8786e76",
     "showTitle": false,
     "title": ""
    },
    "id": "jQ9iKyQlC7Cz"
   },
   "source": [
    "<a name=\"7\"></a>\n",
    "### 7. Projektverlauf und Änderungen\n",
    "In der folgenden Beschreibung erhalten Sie eine ausführliche, zeitliche Zusammenfassung von unserem Projektablauf.\n",
    "\n",
    "__Woche vom 02.05 - 08.05.2022__\n",
    "\n",
    "Begonnen wurde damit, sich über mögliche Datensätze für das Tranieren und Testen des Modells zu informieren. Dabei wurden drei mögliche Datensätze gefunden: iSarcasm, Sarcasm on Reddit und News Headlines Sarcasm. Schlussendlich haben wir uns für den Datensatz von Sarcasm on Reddit entschieden. Des Weiteren wurde sich die Bedeutung und der Aufbau von NGrams angeschaut und ein Skript geschrieben, welches die ersten NGrams aus einer vorgebenen Anzahl von Daten erstellt. Um sich auf den Kurzvortrag vorzubereiten wurden sich die Grundlagen zum Kernel-Trick, Ridge Regression und im Allgemeinen zur Kernel Ridge Regression angeeignet.\n",
    "\n",
    "__Woche vom 09.05 - 15.05.2022__\n",
    "\n",
    "In dieser Woche wurden die Folien des Kurzvortrages fast fertig gestellt. Es musste nur noch auf die Laufzeit eingegangen werden und die ganze Methode mit einem geeigneten Beispiel veranschaulicht werden. Es wurde eine eine Web-App programmiert, die später beim finalen Vorstellen live das Ergebnis zeigt. Dabei wird das entsprechende Python-Skript mit dem Modell aufgerufen, welches die Ergebnisdaten der Web-App übergibt. Um nicht nur mit NGrams zu arbeiten wurden beim Datensatz noch die Wahrscheinlichkeiten der Max, Min und Mean des TFIDF Wert aufgenommen.\n",
    "\n",
    "__Woche vom 16.05 - 22.05.2022__\n",
    "\n",
    "Es wurde das erste Modell gebaut und getestet. Dabei wurden als Features nur die unterschiedlichen TFIDF-Werte (Max, Min, Mean) genutzt. Die Daten sind dabei gelabelt gewesen, wobei 1 dabei für Sarkasmus steht. Dabei wurde sich auch nochmal um die mögliche semantische Vergleichbarkeit von Sätzen beschäftigt. Zusammengefasst war das erste Modell nicht überzeugend, wobei wir uns in der darauffolgenden Woche nochmal mit weiteren möglichen Features beschäftigt haben.\n",
    "\n",
    "__Woche vom 23.05 - 29.05.2022__\n",
    "\n",
    "In der vierten Woche haben wir nochmal eine grobe Übersicht von dem geplanten Aufbau des Modells erstellt. Dabei soll auf dem eingegebenen Text Stemming, NGrams und ein Vokabular angewendet, um das bestmögliche Ergebnis zu bekommen. Außerdem wurden sich weitere Gedanken über mögliche Features gemacht. Die enthaltenen Smileys könnten im Zusammenhang mit dem inhaltlichen Content Aufschluss über Sarkasmus geben. Dabei würde über den Content definiert werden, ob der Inhalt eines Kommentars positiv, negativ der neutral ist. Zusätzlich wurde die einfache Bag of Words Methode ohne Stemming ausprobiert. Dabei wurde mit dem Ridge Classifier auf 50% mit einer Abweichung von +-10 erreicht.\n",
    "\n",
    "__Woche vom 30.05 - 05.06.2022__\n",
    "\n",
    "Wir haben in dieser Woche mit dem Notebook angefangen, welches unseren kompletten Projektverlauf und das erarbeitete Modell beschreibt. Zudem wurde die Präsentation für den Kurzvortrag vorbereitet, welche am Dienstag vorgestellt wurde. Dabei wurde die allgemeine Methode der Kernel Ridge Regression näher erläutert.\n",
    "\n",
    "__Woche vom 06.06 - 12.06.2022__\n",
    "\n",
    "Nachdem die Methode der Kernel Ridge Regression vorgestellt wurde, wurde sich wieder intensiver um das Projekt gekümmert. Dabei wurde weiter am Notebook bzw. der Dokumentation geschrieben. Außerdem wurde dem Modell ein Vokabular hinzugefügt. Dabei hatte das Vokabular im ersten Versuch einen Umfang von 27.000 Wörtern. Jedoch war das Training ziemlich zeitintensiv und es wurde am Ende ein Ergebnis von 39% erzielt. Um die besten Parameter herauszufinden wurde auch Hyperparametertuning angewendet.\n",
    "\n",
    "__Wochen vom 13.06 - 26.06.2022__\n",
    "\n",
    "Durch weitere Änderung der Parameter durch das Hyperparametertuning und einigen Anpassungen im Vokabular kommt das Modell derzeit auf ca. 60%. Im Fokus stand jedoch in den Wochen die Dokumentation, die das Modell und den allgemeinen Projektverlauf beschreibt. Dabei wird jeder Schritt bei der Entwicklung und der Anpassung des Modells beschrieben und grundlegende Begriffe (z.B. Kernel-Trick oder Ridge Regression) definiert."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "L&SC_Übung_SS2022",
   "notebookOrigID": 4378794509398995,
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [],
   "name": "L&SC_Übung_SS2022.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c430a503694e6fed73ff44083efba88bedc6f1290fd7f1bbd6dc6d0c271a396a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
