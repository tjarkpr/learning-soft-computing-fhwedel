{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f2c5a412-3ff4-4d48-a48b-c306ff7639ff",
     "showTitle": false,
     "title": ""
    },
    "id": "xU7S8_kzC7Cd"
   },
   "source": [
    "# Machine Learning und Softcomputing Assessment\n",
    "#### **Sarkasmuserkennung mittels Kernel Ridge Regression**\n",
    "*Die Ergebnisse dieses Notebooks wurden im Rahmen des Assessments Machine Learning and Softcomputing erarbeitet.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f1db978f-b2f2-4d8a-bfdf-92074244ad69",
     "showTitle": false,
     "title": ""
    },
    "id": "9l5KpFk1C7Cg"
   },
   "source": [
    "### Aufgabenstellung\n",
    "Das Themengebiet **Erkennung von Sarkasmus in Text** ist ein h√§ufig erforschtes Thema. Es f√§llt in die Kategorisierung **Semantische Textklassifizierung**, welche bereits mehrere Ans√§tze enth√§lt, um Features aus Texten zu extrahieren. Die Aufgabe stellt sich zusammen aus der Entwicklung eines von qualitativen Features aus einem selbst ausgew√§hlten Datensatz, zum trainieren eines Modells, welches im Anschluss die Texteingabe nach Sarkasmus oder nicht Sarkasmus klassifiziert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "321e162e-e8f4-4b3c-bd70-4c0b709f61ab",
     "showTitle": false,
     "title": ""
    },
    "id": "MFmGJeviC7Ch"
   },
   "source": [
    "### Formelverzeichnis\n",
    "\n",
    "|Variablen|Beschreibung|\n",
    "|---------|------------|\n",
    "|ùë§|Gewichtsmatrix/-vektor\n",
    "|ùë•|Datenpunkt (Training)\n",
    "|z|Datenpunkt (Input)\n",
    "|ùëè|lineare Konstante\n",
    "|Œ±|Korrekturwert\n",
    "|ùúô(ùë•)|Feature-Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "569ea7e2-1434-4718-9352-ed525eb4a0eb",
     "showTitle": false,
     "title": ""
    },
    "id": "bVzEJJsiC7Ci"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "### 1. Einf√ºhrung in Kernel Ridge Regression\n",
    "Kernel Ridge Regression (KRR) ist ein Verfahren der Support Vector Machines (SVM), bei dem die klassische Regression oder Klassifikation um die L2-Regularisierung und Kernels erweitert wird. Durch die Kernels kann das Verfahren den sogenannten Kernel-Trick nutzen, um Rechenzeit einzuspaaren. Kernel Ridge Regression verh√§lt sich um Vergleich zu klassischen Support Vector Machines effizienter und genauer f√ºr Datens√§tze mit einer ` Datenpunktanzahl < 25.000 `. Zudem k√∂nnen hochdimensionale Feature-Vektoren durch den Kernel-Trick wesentlich effizienter oder gar √ºberhaupt genutzt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ce4704f-15fb-45dd-aba9-b385025cbaf7",
     "showTitle": false,
     "title": ""
    },
    "id": "kEr4-a7yC7Ci"
   },
   "source": [
    "<a name=\"1.1\"></a>\n",
    "#### 1.1. SVM mittles Kerneln\n",
    "Eine klassiche lineare Support Vector Machine prognostiziert Ergebnisse anhand der Formel: \n",
    "\n",
    "> `ùëì(z) = <ùë§, z> + ùëè`\n",
    "\n",
    "Da die Datenpunkte in der Regel hochdimensional sind, entsprechen ùë§ und ùë• Vektoren oder Matrizen, f√ºr die das innere Produkt gebildet werden muss, um eine Multiplikation zu erreichen. Die klassische Formel, bzw. die Gewichtsmatrix kann durch den sog. Perzeptron-Algorithmus weiter aufgeschl√ºsselt werden.\n",
    "\n",
    "> `w = ‚àë ùõº * ùë¶ * ùë•)`\n",
    "\n",
    "Der Perzeptron-Algorithmus betrachtet hierbei alle Trainingsdatenpunkte und ihre Ergebnisse und kombiniert diese mit einen Korrekturwert. Somit k√∂nnen die Gewichte iterativ angepasst werden und verlieren dennoch nicht an Aussagekraft im Training durch das zwischenspeichern der Trainingsdaten.<br><br>\n",
    "Weiterf√ºhrend wird das sog. Feature-Mapping angewandt. Das Feature-Mapping ist eine unbekannte Funktion, die einen Datenpunkt in h√∂here Dimensionen √ºberf√ºhrt. Dies ist vorallem wichtig, um noch nicht herausgestellte Zusammenh√§nge abzubilden. Das Feature-Mapping l√§sst sich hierbei auf die, aus dem Perzeptron-Algorithmus resultierende, Interpolationfunktion einbinden. Hierzu wird das innere Produkt des Gewichts- und Inputdatenverktors aufgeschl√ºsselt. Der Gewichtsvektor wird ersetzt und l√§sst eine Verkn√ºpfung der Trainingsdaten mit den Inputdaten zu (inneres Produkt). Auf die Datenpunkte des inneren Produktes l√§sst sich anschlie√üend das Feature-Mapping anwenden.\n",
    "\n",
    "> `ùëì(ùë•) = ‚àë ùõº * ùë¶ * <ùúô(ùë•),ùúô(z)> + ùëè`\n",
    "\n",
    "Das hierbei entstandene innere Produkt l√§sst sich nun durch die Einf√ºhrung des sog. Kernels ersetzen. Ein Kernel bildet dieses innere Produkt mit den Feature-Mapping ab.\n",
    "\n",
    "> `ùëì(ùë•) = ‚àë ùõº * ùë¶ * ùëò(ùë•,z) + ùëè`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "21b7eaa0-1f03-46ab-993c-47a91e1afa87",
     "showTitle": false,
     "title": ""
    },
    "id": "Qbg5ADrxC7Cj"
   },
   "source": [
    "#### 1.2. Kernel-Trick\n",
    "Der Kernel-Trick bezieht sich auf die im vorangegangenen Schritt eingef√ºhrten Kernel in den SVM-Funktionen. Durch den Trick k√∂nnen die inneren Produkte mittels vordefinierten Kernel-Methoden im Input-Space durchgef√ºhrt werden. Hiervon profitiert voallem die Laufzeit und Speicherkomplexit√§t, da die Umwandlung (Feature-Mapping) in den Feature-Space entf√§llt. \n",
    "\n",
    "> **Linearer Kernel:**  `<ùë•,ùë¶>`<br>\n",
    "> **Polynominaler Kernel:**  `(<ùë•,ùë¶>+ ùëê)^ùëë`<br>\n",
    "\n",
    "\n",
    "> **Achtung:** Hierbei muss jedoch darauf geachtet werden, dass die Ersetzung der Umwandlung nur f√ºr bestimmte Funktionen m√∂glich ist. Konkret gilt [Mercer's Theorem](https://)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "db3852a3-eb8d-4545-b2b7-b80c201477ec",
     "showTitle": false,
     "title": ""
    },
    "id": "ml2Ewt27C7Ck"
   },
   "source": [
    "#### 1.3. L2-Regularisierung\n",
    "Die L2-Regularisierung ist ein anderer Begriff f√ºr die Ridge Regression, die in diesem Verfahren in Kombination mit dem Kernel-Trick angewandt wird. \n",
    "\n",
    "Es gibt verschiedene Verfahren der Regression, die bei verschiedenen Szenarien eingesetzt werden k√∂nnen. Die Bekannteste ist dabei wohl die einfache lineare Regression, welche eine statische Methode ist, die die Beziehung zwischen einer Antwortvariablen ` y ` und einer Pr√§diktorvariablen ` x ` beschreibt.\n",
    "\n",
    "> ` y = a + b * x `\n",
    "\n",
    "Wenn in einem Modell mehrere Pr√§diktorvariablen vorkommen, die jeweils voneinander unabh√§ngig sind und eine Beziehung zu der Antwortvariablen haben, wird die multiple lineare Regression eingesetzt. Es gibt dabei wieder eine Antwortvariable ` y ` und ` p ` Pr√§diktorvariablen, die mit ` x(p) ` beschrieben werden. Hinzukommt kommt ` Œ≤(p) `, welche den durschnittlichen Effekt einer Zunahme von ` x(p) ` beschreibt, wenn eine Einheit auf ` y ` gerechnet wird. Alle andere Pr√§diktoren werden dabei festgehalten. \n",
    "\n",
    "> ` y = Œ≤(0) + Œ≤(1) * x(1) + ... + Œ≤(p) * x(p) + Œµ ` \n",
    ">\n",
    "> **Minimierung von** ` RSS`\n",
    "\n",
    "Da es jedoch bei der multiplen linearen Regression zu Multikollinarit√§t kommen kann, wenn zwei oder mehrere Pr√§diktorvariablen miteinander in Beziehung stehen, wird die Ridge Regression eingesetzt. Die multiple lineare Regression versucht, die Summe der quadratischen Residuen zu minimieren. \n",
    "\n",
    "Bei der Ridge Regression kommt ein Strafterm hinzu und wird minimiert. Dieser h√§ngt besonders von dem Hyperparameter ` Œª ` ab. Wenn ` Œª=0 ` ist, dann gibt es keine Ver√§nderung. Je gr√∂√üer ` Œª ` jedoch wird, desto einflussreicher ist ihr Schrumpfungsgrad. ALlgemeien schrumpfen die Pr√§diktorvariablen, die am wenigsten Einfluss haben, am schnellsten gegen 0. \n",
    "\n",
    "> **Minimierung von** ` RSS + Œª * ‚àë * Œ≤(j)¬≤ `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c9310b6-7c03-4b4c-a07b-8e245898383a",
     "showTitle": false,
     "title": ""
    },
    "id": "QnfQJOF8C7Cl"
   },
   "source": [
    "### 2. Auswahl des Datensatzes\n",
    "Anhand einer vorangegangen Internetrecherche sind wir auf die folgenden drei Datens√§tze gesto√üen, die aus unterschiedlichen Erhebungsstandorten eine Klassifikation nach Sarkasmus und nicht Sarkasmus enthielten.\n",
    "Au√üerdem ist es wichtig, dass die Datens√§tze bereits balanciert wurden, ansonsten muss ein weiterer Balancierungsprozess vorgeschoben werden.\n",
    "\n",
    "Nach der Evaluation der Vor- und Nachteile der Datens√§tze, kamen wir zum Ergebnis, dass der Datensatz **Sarcasm on Reddit** am repr√§sentativsten den modernen Sarkasmus darlegt.\n",
    "<br><br>\n",
    "\n",
    "Datensatz|Vorteile|Nachteile\n",
    "---------|--------|---------\n",
    "[iSarcasm](https://)||\n",
    "[Sarcasm on Reddit](https://)|- modernes Sarkasmusverst√§ndnis<br>- gro√üe Anzahl an Datenpunkten|- viele numerische Kontexte<br>- viele Abk√ºrzungen\n",
    "[News Headlines Sarcasm](https://)||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "54e91f27-6f08-43e9-9b2f-f00ae5fdabfa",
     "showTitle": false,
     "title": ""
    },
    "id": "3scoNZ0rC7Cl"
   },
   "source": [
    "#### 2.1. *Sarcasm on Reddit* Metadaten\n",
    ">**Metadaten** = Grundlegende Informationen √ºber die Eintr√§ge des Datensatzes.\n",
    "\n",
    "Metainformation|Wert\n",
    "---------------|----\n",
    "Anzahl an Datenpunkten|~1.000.000 Eintr√§ge\n",
    "Anzahl valider Datenpunkte|962.295 Eintr√§ge\n",
    "Ausgeglichenheitsfaktor|50% Sarkasmus, 50% Kein Sarkasmus\n",
    "Hauptspalten|\"label\", \"comment\", \"parent_comment\"\n",
    "Nebenspalten|\"author\", \"subreddit\", \"score\", \"ups\", \"down\", \"date\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "67dcfb60-7aee-4d2e-a2b8-eeb6727686a9",
     "showTitle": false,
     "title": ""
    },
    "id": "T0EKv9ddC7Cm"
   },
   "source": [
    "---\n",
    "> ### Information zu Databricks:\n",
    "> Um Daten aus dem DBFS zu l√∂schen, kann der folgende Befehl ausgef√ºhrt werden. Wichtig ist, dass erst alle Dateien aus einem Verzeichnis gel√∂scht werden m√ºssen, bevor ein Verzeichnis selbst gel√∂scht werden kann.\n",
    ">```\n",
    ">dbutils.fs.rm(\"/FileStore/tables/...\")\n",
    ">```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e89137bd-04b0-434b-9f1d-bd1afae10aa4",
     "showTitle": false,
     "title": ""
    },
    "id": "zfcEsiyTu9Q1"
   },
   "source": [
    "<a name=\"3\"></a>\n",
    "### 3. Vorbereitung der Trainings-, Validierungs- und Testdaten\n",
    "Um den Datensatz verwenden zu k√∂nnen m√ºssen die Datenpunkte vorbereitet werden. Hierzu z√§hlt das Bereinigen der Daten und das [Feature-Engineering](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114). Aus den Datensatz sollen so viele Informationen wie m√∂glich extrahiert werden. Die Extrahierung zielt vorallem darauf ab die **Informationen zu erlangen**, **die nicht** explizipt **im Datensatz liegen**.<br>\n",
    "In unserem Fall versuchen wir aus einem gro√üen Datensatz an Texten Informationen zu erlangen, die auf semantische Beziehungen schlie√üen lassen, um eine Klassifikation nach Sarkasmus zu realisieren.<br><br>\n",
    "**Enscheidung:** Um einen generalisierten Anastz zu verfolgen, haben wir uns im Vorweg dagegen entschieden die weiteren Variablen des Datensatzes (bspw. *score*, *ups* und *downs*) in unser Modell einzubeziehen. M√∂glicherweise h√§tten diese uns zwar eine genauere Klassifikation erm√∂glicht, jedoch sind diese Informationen f√ºr die sp√§tere generelle Anwendung nicht verf√ºgbar (Eingrenzung des Use-Case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9ea62e63-07db-45c6-b2a1-6910778ea35a",
     "showTitle": false,
     "title": ""
    },
    "id": "-0sGm0Qqc6So"
   },
   "source": [
    "**Initialies Laden des Datensatzes:**<br>\n",
    "*Datensatz wird mittels der Pandas Libary aus der gegebenen CSV-Datei in ein Pandas DataFrame geladen. F√ºr das lokale Ausf√ºhren kann hier der komplette Datensatz eingelesen werden. In Google Colab oder Databricks sollten kleinere Mengen gew√§hlt werden.*<br>\n",
    ">Sollte bereits die Sub-Menge des Datensatzes generiert worden sein, so kann zu <a>Schritt 3.3</a> vorgesprungen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7aa69038-da4b-463a-a534-92fe8f88fae7",
     "showTitle": false,
     "title": ""
    },
    "id": "6sssdGV7cZGB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f3/2fx5j9s944361d_j_4t6jmh40000gn/T/ipykernel_23755/375597757.py:11: DtypeWarning: Columns (0,4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  complete_dataset = pd.read_csv(DATASET_PATH, sep=',', names=HEADER_LIST, encoding=\"ISO-8859-1\", nrows=READ_ROWS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300000 entries, 0 to 299999\n",
      "Data columns (total 10 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   label      300000 non-null  object\n",
      " 1   comment    299994 non-null  object\n",
      " 2   author     300000 non-null  object\n",
      " 3   subreddit  300000 non-null  object\n",
      " 4   score      300000 non-null  object\n",
      " 5   ups        300000 non-null  object\n",
      " 6   downs      300000 non-null  object\n",
      " 7   date       300000 non-null  object\n",
      " 8   ts         300000 non-null  object\n",
      " 9   parent     300000 non-null  object\n",
      "dtypes: object(10)\n",
      "memory usage: 22.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import pyspark\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_NAME = 'dataset.csv'\n",
    "DATASET_PATH = './' + DATASET_NAME\n",
    "\n",
    "READ_ROWS = 300000\n",
    "\n",
    "# For usage via Google Colab:\n",
    "HEADER_LIST = ['label', 'comment', 'author', 'subreddit', 'score', 'ups', 'downs', 'date', 'ts', 'parent']\n",
    "complete_dataset = pd.read_csv(DATASET_PATH, sep=',', names=HEADER_LIST, encoding=\"ISO-8859-1\", nrows=READ_ROWS)\n",
    "\n",
    "# For usage via Databricks:\n",
    "# complete_dataset = sqlContext.read.csv(DATASET_PATH, sep=\",\", header=True, encoding=\"ISO-8859-1\").limit(READ_ROWS)\n",
    "# complete_dataset = complete_dataset.toPandas()\n",
    "\n",
    "print(complete_dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b53a8ec5-f8f9-4496-8c26-6577b8196491",
     "showTitle": false,
     "title": ""
    },
    "id": "5bKPBqm3ZNQX"
   },
   "source": [
    "<a name=\"3.1\"></a>\n",
    "#### 3.1. Bereinigen der Daten\n",
    "Bevor jegliche Art der semeantischen Analyse oder dem Extrahieren einer relevanten und repr√§sentativen Submenge, m√ºssen die Daten bereinigt werden. Es existieren mehrere Fehlerquellen in den Daten, die ohne eine Filtrierung die Egebnisse verf√§lschen w√ºrden.<br>\n",
    "Eine der vielen Fehlerquellen sind None-, NaN-, und leere Werte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3f30964a-a202-4d94-8a3d-0ab220c32fa9",
     "showTitle": false,
     "title": ""
    },
    "id": "mVTLYEIMc2oT"
   },
   "source": [
    "**Analysieren der Daten auf None-, NaN- und leere Werte:**<br>\n",
    "*Gibt die Summe aller None-, NaN- und leeren Werte aus.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3489fdec-45ce-454e-8473-8fbd89ad1d7a",
     "showTitle": false,
     "title": ""
    },
    "id": "a2YzYJwiczUw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summe Na-Werte: 6\n",
      "Summe leere Werte: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sum_na = complete_dataset.isna().sum().sum()\n",
    "sum_empty_string = complete_dataset.eq('').sum().sum()\n",
    "\n",
    "print(\"Summe Na-Werte: \" + str(sum_na))\n",
    "print(\"Summe leere Werte: \" + str(sum_empty_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bb034e9f-b671-4cd5-8bda-b0c0137bdd4a",
     "showTitle": false,
     "title": ""
    },
    "id": "TSyFULU2m0Bg"
   },
   "source": [
    "**Bereinigen der Daten nach None-, NaN- und leeren Werte:**<br>\n",
    "*Entfernt alle None-, NaN- und leeren Werte.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2a6fe308-a2f2-4e06-84b7-9f13d05c4cbc",
     "showTitle": false,
     "title": ""
    },
    "id": "ec9nQ1wqm-JP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summe Na-Werte: 0\n"
     ]
    }
   ],
   "source": [
    "complete_dataset = complete_dataset.dropna()\n",
    "\n",
    "sum_na = complete_dataset.isna().sum().sum()\n",
    "print(\"Summe Na-Werte: \" + str(sum_na))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bc9f1780-784f-4bff-b223-16a9d1600d58",
     "showTitle": false,
     "title": ""
    },
    "id": "0b-lYE10dDYy"
   },
   "source": [
    "**Entfernen von nicht ben√∂tigten Spalten:**<br>\n",
    "*Entfernt alle Spalten bis auf die Kommentare und Label.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "241c9118-b2b5-4f90-9ddb-f22621accaec",
     "showTitle": false,
     "title": ""
    },
    "id": "VwDWrXtxc0rd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['label', 'comment']\n",
      "Size: 299994\n"
     ]
    }
   ],
   "source": [
    "complete_dataset = complete_dataset[['label', 'comment']]\n",
    "\n",
    "print(\"Columns: \" + str(complete_dataset.columns.to_list()))\n",
    "print(\"Size: \" + str(len(complete_dataset.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGfs6n4bC7Cq"
   },
   "source": [
    "**Entfernen von nicht betrachteten Zeichen:**<br>\n",
    "*Entfernt aller Zeichen, die nicht f√ºr die Analyse ben√∂tigt werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5MbPLjuwC7Cq"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_empty(text):\n",
    "    if len(text.split()) == 0:\n",
    "        return ''\n",
    "    return text\n",
    "\n",
    "complete_dataset['comment'] = complete_dataset['comment'].apply(lambda text: re.sub('[^\\.\\?\\!\\,\\;\\sA-Za-z0-9]+', '', text))\n",
    "complete_dataset['comment'] = complete_dataset['comment'].apply(lambda text: replace_empty(text))\n",
    "complete_dataset['comment'] = complete_dataset['comment'].replace('', np.nan)\n",
    "complete_dataset = complete_dataset.dropna(subset=['comment'])\n",
    "complete_dataset = complete_dataset.drop_duplicates(subset=['comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c00d59ee-fe14-4bbe-9d39-00450526a52f",
     "showTitle": false,
     "title": ""
    },
    "id": "wt24e-tnoXGL"
   },
   "source": [
    "<a name=\"3.2\"></a>\n",
    "#### 3.2. Extrahieren einer ausgeglichenen Sub-Menge\n",
    "Da der Datensatz enorm viele Daten enth√§lt, w√§re er f√ºr die weitere Verarbeitung und Aufbereitung insofern ungeeignet, dass er die Rechenleistung enorm beeintr√§chtigt. Zudem steigt die Wahrscheinlichkeit des Overfittings bei steigender Datenpunktanzahl. Au√üerdem ist es wichtig, in diesem Schritt Ausrei√üer zu eleminieren.\n",
    "\n",
    "**Enscheidung:** Wir haben uns f√ºr eine gesamte Datenmenge von ` 6.000 Datenpunkten ` entschieden. Die sich wiederum sp√§ter bei einem ` Train-, Test-Split auf 4.500, 1.500 Datenpunkten `."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2eba6f1a-b218-4504-be6b-e71d91b7cdd1",
     "showTitle": false,
     "title": ""
    },
    "id": "8O-dIDLpC7Cq"
   },
   "source": [
    "**Analysiert, ob ein Kommentar Zahlen enth√§lt:**<br>\n",
    "*Durchl√§uft alle Zeichen der Zeichenfolge, um zu analysieren, ob Zahlen enthalten sind. Sollten diese enthalten sein, werden die Zeilen entfernt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1f74b16e-a86a-4260-99fa-c9d5732be3c2",
     "showTitle": false,
     "title": ""
    },
    "id": "vJJrw75TC7Cq"
   },
   "outputs": [],
   "source": [
    "def contains_digits(comment):\n",
    "    return any(chr.isdigit() for chr in comment)\n",
    "\n",
    "contains_digits_mask = np.vectorize(contains_digits)\n",
    "array_contains_digits = contains_digits_mask(complete_dataset['comment'].values.astype('U'))\n",
    "complete_dataset = complete_dataset[~array_contains_digits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "49d92714-1cf1-4c23-9526-3562a5e6686a",
     "showTitle": false,
     "title": ""
    },
    "id": "q0KrtQMXpOI9"
   },
   "source": [
    "**Analysieren der H√§ufigkeit von W√∂rtern:**<br>\n",
    "*Berechnet die Summer der H√§ufigkeit aller W√∂rter der Datenpunkte.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4e41f9da-03fb-4060-859e-aaf5459cdfd8",
     "showTitle": false,
     "title": ""
    },
    "id": "GWvyTKnfdGxE"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "word_counter = CountVectorizer(lowercase=True, analyzer='word', stop_words='english')\n",
    "array_word_counter = word_counter.fit_transform(complete_dataset['comment'].values.astype('U'))\n",
    "array_word_counter = np.sum(array_word_counter, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1159b2bc-ee93-4143-9f5d-2f218c168731",
     "showTitle": false,
     "title": ""
    },
    "id": "1C9ONxkyC7Cr"
   },
   "source": [
    "**Analysieren der Anzahl an W√∂rtern:**<br>\n",
    "*Berechnet die Anzahl an W√∂rtern der Datenpunkte.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1e2fc6c6-186f-4e94-9d1e-054b84a6c070",
     "showTitle": false,
     "title": ""
    },
    "id": "3CQxHDafC7Cr"
   },
   "outputs": [],
   "source": [
    "def count_words(comment):\n",
    "    return len(comment.split())\n",
    "\n",
    "comment_size_mask = np.vectorize(count_words)\n",
    "array_comment_size = comment_size_mask(complete_dataset['comment'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOXHFCbGC7Cr"
   },
   "source": [
    "**Analysieren der durchschnittlichen Wortgr√∂√üe:**<br>\n",
    "*Berechnet die durchschnittliche Wortgr√∂√üe der Datenpunkte.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "V1v_PtcyC7Cr"
   },
   "outputs": [],
   "source": [
    "def measure_words(comment):\n",
    "    words = comment.split()\n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "word_size_mask = np.vectorize(measure_words)\n",
    "array_word_size = word_size_mask(complete_dataset['comment'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8fbb9fe2-9230-4a71-8a2b-0250ed1012ba",
     "showTitle": false,
     "title": ""
    },
    "id": "Lv5d36_2C7Cr"
   },
   "source": [
    "**Konkatenieren der gesammelten Daten:**<br>\n",
    "*Zusammenf√ºhren der Daten und in einen Pandas Dataframe √ºberf√ºhren. Die Anzahl und H√§ufigkeit der W√∂rter werden an die Datenpunkte angeh√§ngt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dbc9e152-1aea-4cc1-a39d-b96c98cebb05",
     "showTitle": false,
     "title": ""
    },
    "id": "yjVNZOXjC7Cs"
   },
   "outputs": [],
   "source": [
    "concatenated_data = np.array(np.hstack((\n",
    "        np.reshape(complete_dataset['label'].values, (-1, 1)),\n",
    "        np.reshape(complete_dataset['comment'].values, (-1, 1)),\n",
    "        np.reshape(array_comment_size, (-1, 1)),\n",
    "        np.reshape(array_word_counter, (-1, 1)),\n",
    "        np.reshape(array_word_size, (-1, 1)),\n",
    ")))\n",
    "concatenated_data = pd.DataFrame(concatenated_data, columns=['label', 'comment', 'comment_size', 'word_count', 'avg_word_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad019dac-5fe8-407c-ae43-ce2386298921",
     "showTitle": false,
     "title": ""
    },
    "id": "4kbNHTBuC7Cs"
   },
   "source": [
    "**Filtern und Sortieren der Daten:**<br>\n",
    "*Die Daten werden im Datenframe nach der Anzahl an W√∂rtern gefiltert, wobei diese gegen eine Mindestwortanzahl gepf√ºft werden. Anschlie√üend werden die Daten nach der Anzahl der W√∂rter und dessen H√§ufigkeit sortiert.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "680c2a20-0e6f-482a-bead-4f28d3f13ae4",
     "showTitle": false,
     "title": ""
    },
    "id": "cKbZVJ9hC7Cs"
   },
   "outputs": [],
   "source": [
    "MIN_WORD_COUNT = 4\n",
    "\n",
    "concatenated_data = concatenated_data[concatenated_data['comment_size'] >= MIN_WORD_COUNT]\n",
    "concatenated_data = concatenated_data.sort_values(by=['word_count', 'avg_word_size', 'comment_size'], ascending=[True, False, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "14bfee95-ddf9-428b-afdc-988b5bad7978",
     "showTitle": false,
     "title": ""
    },
    "id": "fqMxRF5XC7Cs"
   },
   "source": [
    "**Extrahieren eines gleich gro√üen Submenge:**<br>\n",
    "*Aus dem Dataframe wird eine gleich gro√üe Submenge an positiven und negativen Sarkasmusbefunden extrahiert und konkateniert (`6.000 Datenpunkte`).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf7582bb-bbc7-4ca3-a193-5cf3641c951c",
     "showTitle": false,
     "title": ""
    },
    "id": "-_mMIiZZC7Cs"
   },
   "outputs": [],
   "source": [
    "SUBSET_PART_SIZE = 6000\n",
    "\n",
    "# Note: For Databricks please use char representation instead.\n",
    "POSITIVE_LABEL = 1\n",
    "NEGATIVE_LABEL = 0\n",
    "\n",
    "concatenated_data_positive = concatenated_data[concatenated_data['label'] == POSITIVE_LABEL].head(SUBSET_PART_SIZE)\n",
    "concatenated_data_negative = concatenated_data[concatenated_data['label'] == NEGATIVE_LABEL].head(SUBSET_PART_SIZE)\n",
    "concatenated_data = pd.concat([concatenated_data_positive, concatenated_data_negative])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f90d60ee-e7cc-4735-9026-1149a82452bf",
     "showTitle": false,
     "title": ""
    },
    "id": "ZeiGBG_qC7Ct"
   },
   "source": [
    "**Abspeichern der neuen Submenge:**<br>\n",
    "*Das Dataframe wird in eine CSV-Datei abgespeichert, um diese im sp√§teren Verlauf zu laden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2cc5db43-8d4b-4d4a-8f71-8cc33c0ef7aa",
     "showTitle": false,
     "title": ""
    },
    "id": "swqOHbQdC7Ct"
   },
   "outputs": [],
   "source": [
    "SUBSET_NAME = 'subset-dataset.csv'\n",
    "OUTPUT_PATH = './' + SUBSET_NAME\n",
    "\n",
    "concatenated_data = concatenated_data.loc[:, ~concatenated_data.columns.isin(['word_count', 'comment_size', 'avg_word_size'])]\n",
    "\n",
    "# For usage via Google Colab:\n",
    "concatenated_data.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "# For usage via Databricks:\n",
    "# sparkDataFrame = spark.createDataFrame(concatenated_data)\n",
    "# sparkDataFrame.coalesce(1).write.csv(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9a11d481-f3fd-40aa-a184-a743c0a88b6e",
     "showTitle": false,
     "title": ""
    },
    "id": "QGGYCmL6C7Ct"
   },
   "source": [
    "---\n",
    "> ### Information:\n",
    "> Die nachfolgende Schritte sollten erst ausgef√ºhrt werden, wenn die Submenge in <a>Schritt 3.2</a> bereits in eine CSV-Datei gespeichert wurde. Diese CSV-Datei soll im folgenden als neue Datenquelle fungieren und muss dementsprechend erst eingelesen werden Sollte das Vocabular auch bereits vorliegen, so kann zu <a>Schritt 3.5</a> gesprungen werden.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9e227689-6c53-458b-ab45-727b28de7190",
     "showTitle": false,
     "title": ""
    },
    "id": "khZ4COJXC7Ct"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12000 entries, 0 to 11999\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   label    12000 non-null  int64 \n",
      " 1   comment  12000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 187.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import pyspark\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_NAME = 'subset-dataset.csv'\n",
    "DATASET_PATH = './' + DATASET_NAME\n",
    "\n",
    "# For usage via Google Colab:\n",
    "HEADER_LIST = ['label', 'comment']\n",
    "complete_dataset = pd.read_csv(DATASET_PATH, names=HEADER_LIST, header=0)\n",
    "\n",
    "# For usage via Databricks:\n",
    "# complete_dataset = sqlContext.read.csv(DATASET_PATH, header=False).toDF('label', 'comment')\n",
    "# complete_dataset = complete_dataset.toPandas()\n",
    "\n",
    "print(complete_dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "37a83d3c-de17-4752-98dd-b52935809ff8",
     "showTitle": false,
     "title": ""
    },
    "id": "gLs7aW7hC7Ct"
   },
   "source": [
    "<a name=\"3.3\"></a>\n",
    "#### 3.3. Lemmatizing der Texte\n",
    "Das <a>Lemmmatizing</a> setzt alle W√∂rter auf ihren Wortstamm und verringert somit die Anzahl an einzigartigen Werten zwischen den Texten. Dies ist vorallem wichtig, um im sp√§teren Verlauf die W√∂rterh√§ufigkeit im gesamten Dokument einheitlich bemessen zu k√∂nnen. Au√üerdem kann anhand der verk√ºrzten W√∂rter ein wesentlich effizienteres Vokabular aufgebaut werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3842f11a-e3c2-4b29-8be6-3cf629d132ef",
     "showTitle": false,
     "title": ""
    },
    "id": "tO2rhkhtC7Ct"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/fprinz/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/fprinz/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/fprinz/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/fprinz/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def lemmatize_words(comment):\n",
    "    word_list = pos_tag(tokenizer.tokenize(comment))\n",
    "    lemmatize_words = []\n",
    "    for word, tag in word_list:\n",
    "        wtag = tag[0].lower()\n",
    "        if wtag in ['a', 'r', 'n', 'v', 's']:\n",
    "            word = lemmatizer.lemmatize(word, wtag)\n",
    "        lemmatize_words.append(word)\n",
    "    return ' '.join(lemmatize_words)\n",
    "\n",
    "lemmatize_words_mask = np.vectorize(lemmatize_words)\n",
    "complete_dataset['comment'] = lemmatize_words_mask(complete_dataset['comment'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a9ca3488-0ebd-4777-bcd9-72e28ac4c436",
     "showTitle": false,
     "title": ""
    },
    "id": "_-wddXg8C7Cu"
   },
   "source": [
    "**Was ist ein Tokenizer?**<br>\n",
    "Ein Tokenizer dient zur Auftrennung von Zeichenfolgen basierend auf festgelegten Regeln. Je nach Implementierung kann der Tokenizer rudiment√§re Wortzerlegung durchf√ºhren oder basierend auf der jeweiligen Sprache spezifische Worttrennungen durchf√ºhren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "83f662b8-d108-448e-9c62-7acd08daaa4e",
     "showTitle": false,
     "title": ""
    },
    "id": "6FMgb1A2C7Cu"
   },
   "source": [
    "**Was sind Pos-Tags?**<br>\n",
    "Die Pos-Tags geben an, um welche Art von Wort es sich im Kontext des Satzes handelt. Unterschieden wird beispielsweise nach *Nomen*, *Verben* oder *Subjekten*. Somit kann der Lemmatizer die Wortst√§mme besser identifizieren und die Anzahl an Fehlbildungen geht zur√ºck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "60357663-1215-4c25-928d-4163c99c76ef",
     "showTitle": false,
     "title": ""
    },
    "id": "AsRcgzo4C7Cu"
   },
   "source": [
    "**Was ist der Unterschied zwischen Stemming und Lemmatizing?**<br>\n",
    "*Das Stemming schneidet Konjugationen einzelner W√∂rter ab und verringert somit die Anzahl an einzigartigen Werten zwischen den Texten. Da hierbei allerdings nicht immer der Wortstamm zur√ºckgegeben wird, leidet die Vergleichbarkeit der Texte. Da das Stemming im Vergleich zum Lemmatizing lediglich die Konjugation abschneidet und das Wort nicht auf den Wortstamm zur√ºck bildet, kann man sagen, dass das Stemming vergleichsweise rudiment√§r funktioniert.*<br>\n",
    "```\n",
    "stemmer = PorterStemmer()\n",
    "word_stemmed = stemmer.stem(\"studies\")\n",
    "> studi\n",
    "```\n",
    "```\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "word_lemmatized = lemmatizer.lemmatize(\"studies\")\n",
    "> study\n",
    "```\n",
    "<br>**Entscheidung:** Da es uns um die gr√∂√üt m√∂gliche semantische Korrektheit bei hoher Effizienz geht, haben wir uns f√ºr das Lemmatizing entschieden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "26da9681-19f2-4702-a701-6688060a00a9",
     "showTitle": false,
     "title": ""
    },
    "id": "GrMs02wQC7Cu"
   },
   "source": [
    "**Entfernen der Punktuationen:**<br>\n",
    "*F√ºr die Lemmatization ist es wichtig die Satztrennzeichen in den Kommentaren bei zu behalten. Diese k√∂nnen nun entfernt werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a202571e-5e30-4f5e-87de-e0628386ec5a",
     "showTitle": false,
     "title": ""
    },
    "id": "bQ-vcRX5C7Cu"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    if type(text) != str or len(text) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    clean_text = \"\"\n",
    "    for char in text:\n",
    "        if char not in string.punctuation:\n",
    "            clean_text += char\n",
    "    return clean_text\n",
    "\n",
    "complete_dataset['comment'] = complete_dataset['comment'].apply(lambda text: remove_punctuation(text))\n",
    "complete_dataset = complete_dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "09e11fec-67da-4ca6-a4db-9466822977f3",
     "showTitle": false,
     "title": ""
    },
    "id": "NWphm1nFC7Cu"
   },
   "source": [
    "**Abspeichern der neuen Datenmenge:**<br>\n",
    "*Das Dataframe wird in eine CSV-Datei abgespeichert, um diese im sp√§teren Verlauf zu laden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5b006cb9-dd91-4c2c-a45a-fd37282fa4dd",
     "showTitle": false,
     "title": ""
    },
    "id": "3taepByGC7Cu"
   },
   "outputs": [],
   "source": [
    "SUBSET_NAME = 'prepared-dataset.csv'\n",
    "OUTPUT_PATH = './' + SUBSET_NAME\n",
    "\n",
    "# For usage via Google Colab:\n",
    "complete_dataset.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "# For usage via Databricks:\n",
    "# sparkDataFrame = spark.createDataFrame(complete_dataset)\n",
    "# sparkDataFrame.coalesce(1).write.csv(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6d0ee927-72c4-4d6e-904a-c9bf41f8c024",
     "showTitle": false,
     "title": ""
    },
    "id": "80WvzeN1C7Cu"
   },
   "source": [
    "<a name=\"3.4\"></a>\n",
    "#### 3.4. Erstellen eines Bag-Of-Words Vocabulars\n",
    "In diesem Schritt soll die Submenge der Datenbasis in ein <a>Vocabular</a> aufgebrochen werden, welche sich aus den <a>Bag-Of-Words</a> ergeben.\n",
    "Daf√ºr benutzen wir einen <a>CountVectorizer</a> und extrahieren die trainierten Features, die aus sogenannten <a>N-Grams</a> bestehen. Diese k√∂nnen wir dann als Array in eine CSV-Datei abspeichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df1c038a-6f9c-4932-9642-e427907f2ecf",
     "showTitle": false,
     "title": ""
    },
    "id": "pL2WatnbC7Cv"
   },
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "VOCABULARY_NAME = 'vocabulary.csv'\n",
    "OUTPUT_PATH = './' + VOCABULARY_NAME\n",
    "NGRAM_RANGE = (1,3)\n",
    "\n",
    "count_vectorizer = CountVectorizer(lowercase=True, analyzer='word', stop_words='english', ngram_range=NGRAM_RANGE)\n",
    "count_vectorizer.fit_transform(complete_dataset['comment'].values.astype('U'))\n",
    "\n",
    "# Note: For newer versions of scikit-learn use get_feature_names_out().\n",
    "\n",
    "# For usage via Google Colab:\n",
    "df_vocabulary = pd.DataFrame(count_vectorizer.get_feature_names_out())\n",
    "df_vocabulary.to_csv(OUTPUT_PATH, index=False, header=False)\n",
    "\n",
    "# For usage via Databricks:\n",
    "# df_vocabulary = pd.DataFrame(count_vectorizer.get_feature_names())\n",
    "# sparkDataFrame = spark.createDataFrame(df_vocabulary)\n",
    "# sparkDataFrame.coalesce(1).write.csv(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2cb34c8f-3d83-468e-ac18-253e56748ddb",
     "showTitle": false,
     "title": ""
    },
    "id": "MfjIryZ7C7Cv"
   },
   "source": [
    "**Was ist Bag-Of-Words?**<br>\n",
    "Die Bag-of-Words Methode dient im Bereich des Language Processing der Erzeugung nummerischer Features aus einer gegebenen Menge an textuellen Eingaben. Dazu wird zun√§chst ein Vokabular aus der gegebenen Menge an Eingabetexten erzeugt (siehe unten).\n",
    "Die Bag-of-Words Repr√§sentation eines Satzes gibt an, aus welchen W√∂rtern sich der Satz zusammensetzt. Sie gibt dabei allerdings keine verl√§ssliche Auskunft dar√ºber, wie diese angeordnet sind. Vielmehr besteht sie aus einem Vektor mit der Dimension n (n=L√§nge des Vokabulars). Jedes einzelne Merkmal des Vektors gibt dabei an, ob das durch das Merkmal repr√§sentierte Wort des Vokabulars teil dies Satzes ist.\n",
    "\n",
    "<u>Beispiel:</u>\n",
    "\n",
    "Satz 1: Das ist toll.\n",
    "\n",
    "Satz 2: Das ist schlecht.\n",
    "\n",
    "Vokabular (Ohne Interpunktion und Gro√üschreibung)= [das; ist; toll; schlecht]\n",
    "\n",
    "B-o-W 1: (1; 1; 1; 0)\n",
    "\n",
    "B-o-W 1: (1; 1; 0; 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "757ffbb8-2ef3-40e2-bc65-66951431185f",
     "showTitle": false,
     "title": ""
    },
    "id": "8msk5cXpC7Cv"
   },
   "source": [
    "**Was sind N-Grams?**<br>\n",
    "N-Grams sind eine M√∂glichkeit den Zusammenhang zwischen verschiedenen Worten zu analysieren. Dies geschieht durch eine gemeinsame Erfassung verschiedener Textabschnitte, deren L√§nge vom Anwender definiert werden kann.\n",
    "\n",
    "<u>Beispiel</u><br>\n",
    "\"Dieser Satz dient als Beispiel f√ºr NGrams.\"<br>\n",
    "\n",
    "NGrams (Ohne Interpunktion und Gro√üschreibung)<br>\n",
    "\n",
    "Monograms: [\"dieser\"; \"satz\"; \"dient\"; \"als\"; \"beispiel\"; \"f√ºr\"; \"ngrams\"]<br>\n",
    "Bigrams: [\"dieser satz\"; \"satz dient\"; \"dient als\"; \"als beispiel\"; \"beispiel f√ºr\"; \"f√ºr ngrams\"]<br>\n",
    "Trigrams: [\"dieser satz dient\"; \"satz dient als\"; \"dient als beispiel\"; \"als beispiel f√ºr\"; \"beispiel f√ºr ngrams\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "29610614-9faa-42cb-915c-d8674fa52548",
     "showTitle": false,
     "title": ""
    },
    "id": "bUC-Dp--C7Cv"
   },
   "source": [
    "**Was ist ein Vokabular?**<br>\n",
    "Ein Vokabular bezeichnet die Zuordnung von Texttermen auf die Indizes eines numerischen Featurevektors. Sollte ein Vokabular nicht bei der Instanziierung √ºbergeben werden, ist es den FeatureExtraktoren von SKlearn m√∂glich, basierend auf den Trainingsdaten eines zu erzeugen.<br>\n",
    "\n",
    "<u>Beispiel:</u><br>\n",
    "\"Dieser Satz dient als Beispiel f√ºr ein Vokabular.\"<br>\n",
    "Vokabular (ohne Interpunktion und Gro√üschreibung): {\"dieser\": 0; \"satz\": 1; \"dient\": 2; \"als\": 3; \"beispiel\": 4; \"f√ºr\": 5; \"ein\": 6; \"vokabular\": 7}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d54922c9-c835-4ce6-9f80-c70b53fb96e8",
     "showTitle": false,
     "title": ""
    },
    "id": "pkaqDlh9C7Cv"
   },
   "source": [
    "**Was ist ein CountVectorizer?**<br>\n",
    "Der von SKlearn zur Verf√ºgung gestellte CountVectorizer stellt eine M√∂glichkeit dar, aus Texten in String-Repr√§sentation numerische Vektoren zu generieren.\n",
    "Hierzu lernt der Vecotrizer zun√§chst eine gegebene Menge an Texten um ein Vokabular zu erzeugen, auf dessen Basis er im Folgenden die Transformation der gelieferten Texte in Vektoren vornehmen kann. Alternativ kann dieses Vokabular √ºber den *vocabulary*-Parameter an den Vectorizer √ºbergeben werden.\n",
    "Sobald der *fitting*-Vorgang beendet ist, ist der Vectorizer in der Lage Texte zu konvertieren. Das Ergebnis ist ein Vektor der Dimension n (n=L√§nge des gelernten Vokabulars) dessen einzelne Merkmale jeweils die Anzahl an Auftritten des durch den Index i repr√§sentierten Wertes des Vokabulars im Text darstellen.<br>\n",
    "```\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "```\n",
    "Der erzeugte Vectorizer wurde im obigen Beispiel dahingehend parametrisiert, dass er alle gegeben Texteingaben zu Kleinbuchstaben konvertiert sowie die englischen Stopwords entfernt. Bei sogenannten Stopwords handelt es sich um W√∂rter, die in einer gegebenen Sprachen (hier Englisch) keine Bedeutung innerhalb eines Satzes hinzuf√ºgen. Dazu geh√∂ren unter anderem 'the', 'it', 'am' etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6bb533b6-5f4f-408e-83c7-afb6d9e9d96e",
     "showTitle": false,
     "title": ""
    },
    "id": "w6ggd3XvC7Cv"
   },
   "source": [
    "---\n",
    "> ### Information:\n",
    "> Die nachfolgende Schritte sollten erst ausgef√ºhrt werden, wenn die Submenge in <a>Schritt 3.2</a> und das Vocabular in <a>Schritt 3.4</a> bereits in eine CSV-Datei gespeichert wurde. Diese CSV-Dateien soll im folgenden als neue Datenquelle fungieren und m√ºssen dementsprechend erst eingelesen werden.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba05cbd9-3b0c-4b34-aeeb-ace6f59a5389",
     "showTitle": false,
     "title": ""
    },
    "id": "To8pvk8jC7Cv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12000 entries, 0 to 11999\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   label    12000 non-null  int64 \n",
      " 1   comment  12000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 187.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13049 entries, 0 to 13048\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       13049 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 102.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import pyspark\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_NAME = 'prepared-dataset.csv'\n",
    "DATASET_PATH = './' + DATASET_NAME\n",
    "VOCABULARY_NAME = 'vocabulary.csv'\n",
    "VOCABULARY_PATH = './' + VOCABULARY_NAME\n",
    "\n",
    "# For usage via Google Colab:\n",
    "HEADER_LIST = ['label', 'comment']\n",
    "complete_dataset = pd.read_csv(DATASET_PATH, names=HEADER_LIST, header=0)\n",
    "vocabulary = pd.read_csv(VOCABULARY_PATH, header=None)\n",
    "\n",
    "# For usage via Databricks:\n",
    "# complete_dataset = sqlContext.read.csv(DATASET_PATH, header=False).toDF('label', 'comment')\n",
    "# complete_dataset = complete_dataset.toPandas()\n",
    "# vocabulary = sqlContext.read.csv(VOCABULARY_PATH, header=False)\n",
    "# vocabulary = vocabulary.toPandas()\n",
    "\n",
    "print(complete_dataset.info())\n",
    "print(vocabulary.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "01e60dc9-a7bd-495a-9790-494f78753b3d",
     "showTitle": false,
     "title": ""
    },
    "id": "kcqiTG7XC7Cw"
   },
   "source": [
    "<a name=\"3.5\"></a>\n",
    "#### 3.5. Generieren der Trainings- und Testdaten\n",
    "In diesem Schritt werden die Trainings- und Testdaten erst komplett generiert und im Anschluss mit Hilfe der `train_test_split` Methode aufgeteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "856e08eb-e47b-4f2b-886e-0fd6ca52d241",
     "showTitle": false,
     "title": ""
    },
    "id": "3e2Vk4dpC7Cw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Note: Please choose the same NGRAM_RANGE as in the vocabulary.\n",
    "NGRAM_RANGE = (1,3)\n",
    "\n",
    "count_vectorizer = CountVectorizer(lowercase=True, analyzer='word', stop_words='english', ngram_range=NGRAM_RANGE, vocabulary=vocabulary.iloc[:, 0].values.astype('U'))\n",
    "bag_of_words = count_vectorizer.fit_transform(complete_dataset['comment'].values.astype('U'))\n",
    "\n",
    "complete_x = np.array(bag_of_words.todense())\n",
    "complete_y = complete_dataset['label'].values.astype('int32')\n",
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(complete_x, complete_y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "892fca03-897b-4ed9-9d49-97e479e13716",
     "showTitle": false,
     "title": ""
    },
    "id": "eOY0y846C7Cw"
   },
   "source": [
    "<a name=\"4\"></a>\n",
    "### 4. Initialisieren und Trainieren des Modells\n",
    "In diesem Schritt wird die Kernel Ridge Regression erstmalig im Rahmen eines Modells initialisiert. Dieses Modell wird im Anschluss trainiert und durch <a>Hyperparametertuning</a> optimiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d427dfd6-7908-4a6c-8800-b80a2388f5ef",
     "showTitle": false,
     "title": ""
    },
    "id": "I7MbiNMwC7Cw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, estimator=KernelRidge(), n_jobs=2,\n",
       "             param_grid={'alpha': [1, 2, 0.95], 'coef0': [0.95],\n",
       "                         'degree': [2, 1], 'gamma': [0.1, 0.2],\n",
       "                         'kernel': ['poly']},\n",
       "             scoring=make_scorer(score), verbose=4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "def score(y_true, y_pred):\n",
    "    y_pred = np.rint(y_pred)\n",
    "    return f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "scorer = make_scorer(score)\n",
    "\n",
    "model = GridSearchCV(\n",
    "    estimator=KernelRidge(),\n",
    "    param_grid={\n",
    "        'alpha': [1, 2, 0.95], \n",
    "        'degree': [2, 1], \n",
    "        'gamma': [0.1, 0.2], \n",
    "        'coef0': [0.95], \n",
    "        'kernel': ['poly']\n",
    "    },\n",
    "    verbose=4,\n",
    "    scoring=scorer,\n",
    "    n_jobs=2,\n",
    "    cv=2\n",
    ")\n",
    "model.fit(x_train, np.asarray(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6w0cU_FgC7Cw"
   },
   "source": [
    "Der `GridSearchCV` ist eine Klasse zur generischen Optimierung einer Machine Learning Methode anhand von definierten Hyperparametern. √úber den Konstruktorparameter `param_grid` kann ein Dictionary an Paramtern f√ºr den Estimator √ºbergeben werden. Im Dictionary stehen die M√∂glichkeiten der einzelnen Paramter, die auf die beste Kombination unterscuht werden sollen. Au√üerdem k√∂nnen unterschiedliche Kernel angegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "fgAZFVmRC7Cw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter: {'alpha': 0.95, 'coef0': 0.95, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "Erzielter Tuning-Score: 0.5838888888888889\n"
     ]
    }
   ],
   "source": [
    "print('Beste Parameter: ' + str(model.best_params_))\n",
    "print('Erzielter Tuning-Score: ' + str(model.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXj2gPcEC7Cw"
   },
   "source": [
    "Aus dem `GridSearchCV` k√∂nnen die besten Parameter und der beste Score dieser Parameter im Tuning ausgegeben werden. Au√üerdem kann der beste Estimator auf neuen Daten angewandt werden, um bspw. einen Score zu berechnen. Hierbei wird einheitlich auf die im `GridSearchCV` definierte Score-Funktion verwiesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9d81a3b2-e971-4c33-8550-9eddfdcd888f",
     "showTitle": false,
     "title": ""
    },
    "id": "h2MojyXBC7Cw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Score: 0.8877777777777778\n",
      "Test-Score: 0.607\n"
     ]
    }
   ],
   "source": [
    "print(\"Train-Score: \" + str(model.score(x_train, y_train)))\n",
    "print(\"Test-Score: \" + str(model.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "55e8f08f-08e5-48ba-9b25-65789eaa1e5d",
     "showTitle": false,
     "title": ""
    },
    "id": "Xhb8zX7FC7Cx"
   },
   "source": [
    "**Was ist ein Hyperparametertuning?**<br>\n",
    "Als Hyperparametertuning wird die iterative Anpassung der Parameter bezeichnet, die vor dem Trainingsvorgang eines Modells festgelegt werden k√∂nnen. Ihre Wahl kann f√ºr die G√ºte eines Modells von entscheidender Wichtigkeit sein, weswegen ihre Optmimierung ein wichtiger Schritt bei der Modellentwicklung ist.<br>\n",
    "W√§hrend des Optimierungsvorgangs wird das Modell mehrmals mit unterschiedlichen Hyperparametern trainiert um anschlie√üend seine Genauigkeit im Hinblick auf Trainings- und Testdaten zu bestimmen. F√ºr jedes Set an Hyperparametern (im folgenden als Konfiguration bezeichnet) speichert sich das Modell die G√ºteindikatoren um nach Ablauf der Iterationen bestimmen zu k√∂nnen, welche Konfiguration das beste Ergebnis hervorbrachte. Die am besten bewertete Konfiguration wird dann als Resultat des Tunings bereitgestellt.<br><br>\n",
    "Im obigen Beispiel wird das GridSearch Verfahren genutzt. Hierbei ist f√ºr die zu optimierenden Parameter eine Vorauswahl getroffen worden, aus der sich das GridSearch Verfahren Konfigurationen zusammenstellen kann. Somit werden verschiedene Variationen von Hyperparametern √ºberpr√ºft um die beste Konfiguration bestimmen zu k√∂nnen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0c8ec7fc-a639-49b3-bb67-3be0a827664c",
     "showTitle": false,
     "title": ""
    },
    "id": "ItWkbM8kC7Cx"
   },
   "source": [
    "<a name=\"4.1\"></a>\n",
    "#### 4.1. Ergebnisse des Tunings\n",
    "|ID|N|n|NGram|Kernel|Alpha|Train|Test\n",
    "|--|-|-|-----|-----|------|-----|----\n",
    "|1|200.000|1.000|(1,10)|Poly|0.001|0.66|-0.08\n",
    "|2|50.000|1.000|(1,10)|Poly|0.001|0.75|-0.06\n",
    "|3|50.000|10.000|(1,10)|Poly|0.001|0.49|0.09\n",
    "|4|50.000|10.000|(2,3)|Poly|0.001|0.37|0.01\n",
    "|5|50.000|10.000|(1,20)|Poly|0.001|0.49|0.09\n",
    "|6|500|10|(1,10)|Poly|0.001|0.99|-1.47\n",
    "|7|100.000|10.000|(1,10)|Poly(d=2)|0.001|0.41|0.08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnQi1_OtC7Cx"
   },
   "source": [
    "**Umstellung der Scoring-Funktion:**<br>\n",
    "Nach einigen Trainingsdurchl√§ufen ist aufgefallen, dass die Bewertung der ermittelten Werte fehlerhaft ist. Die verwendete Scoring-Funktion hat sich automatisch auf `accuracy` umgestellt. Bei dieser wird die G√ºte anhand der Genauigkeit ermittelt (Abweichung vom tats√§chlichen Wert). Dies ist besonders in der Klassifikation unbrauchbar, da hierbei ein Score der die Klassifizierungsfehler/-quote enth√§lt wesentlich genauer bewertet. Daher haben wir uns f√ºr den `f1_score` entschieden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcaFh3NLC7Cx"
   },
   "source": [
    "|ID|N|n|NGram|Kernel|Alpha|Train|Test\n",
    "|--|-|-|-----|-----|------|-----|----\n",
    "|1|50.000|10.000|(1,10)|Poly|0.001|0.87|0.55\n",
    "|2|100.000|20.000|(1,3)|Poly|0.001|0.81|0.59\n",
    "|3|200.000|5.000|(1,3)|Sigmoid(c0=1.1,g=0.4)|0.1|0.86|0.59\n",
    "|3|200.000|6.000|(1,3)|Poly(c0=0.95,g=0.1)|0.95|0.83|0.58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckcvg9GYC7Cx"
   },
   "source": [
    "**Weitere Erkenntnisse:**<br>\n",
    "Bei `scikit-learn` werden einige Regularien in Implementationen integriert, die bei ineffizienter Nutzung von Modellen greifen. Eine dieser Regularien betrifft die Ridge Regression, die an Effektivit√§t f√ºr Inputdaten mit h√∂herer Dimensionsanzahl als Trainingsdaten verliert. Hier wird automatisch auf eine andere Gewichtungs-/Optimierungsfunktionen umgestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "WB-oYjPrC7Cx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best-model.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Speichern des Modells und des Suchcontainers\n",
    "joblib.dump(model, 'grid-search_model.pkl')\n",
    "joblib.dump(model.best_estimator_, 'best-model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "91a984cc-10cc-4a84-931f-d0fb9d3d4e0b",
     "showTitle": false,
     "title": ""
    },
    "id": "HnEYCp0tC7Cx"
   },
   "source": [
    "<a name=\"4.2\"></a>\n",
    "#### 4.2. Exkurs: Falkon-ML\n",
    "Bei Falkon-ML handelt es sich um eine verbesserte Variante von Kernel Ridge Regression. Um Kernel Ridge Regression zu verbessern kann vorallem die Komplexit√§t des Modells vereinfacht werden. Um dieses Ziel zu erreichen, werden die Kernel-Methoden approximiert. Diese Funktionalit√§t wird vorallem durch das Konzept der Random Projections gegeben. Hierbei handelt es sich um Projektionen in einen Subspace eines Datenraums. Somit werden keine vordefinierten Kernel verwendet, sondern passende zuf√§llige Kernel gew√§hlt. Diese werden dann im Trainingsprozess optimiert. Diese reduzieren die Rechenzeit und erh√∂hen die Flexibilit√§t hinsichtlich hochdimensionalen Daten.<br><br>\n",
    "**Zus√§tzliche Features:**<br>\n",
    "- Full multi-GPU support - All compute-intensive parts of the algorithms are multi-GPU capable.\n",
    "- Extreme scalability - Unlike other kernel solvers, we keep memory usage in check. We have tested the library with datasets of billions of points.\n",
    "- Sparse data support\n",
    "- Scikit-learn integration - Our estimators follow the scikit-learn API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3b60c0f-e7b8-42c6-aafa-37e240a33093",
     "showTitle": false,
     "title": ""
    },
    "id": "b27RhJ-sC7Cx"
   },
   "source": [
    "<a name=\"5\"></a>\n",
    "### 5. Nutzen des Modells\n",
    "Nachdem das Modell trainiert wurde, kann es f√ºr die Vorhersage/Klassifikation genutzt werden. Hierzu m√ºssen alle ben√∂tigten Libaries erneut importiert werden. Au√üerdem muss die Funktionalit√§t der Vorverarbeitung bereitgestellt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "bBjhRixrC7Cy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/fprinz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/fprinz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/fprinz/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/fprinz/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "VOCABULARY_NAME = 'vocabulary.csv'\n",
    "VOCABULARY_PATH = './' + VOCABULARY_NAME\n",
    "NGRAM_RANGE = (1,3)\n",
    "\n",
    "vocabulary = pd.read_csv(VOCABULARY_PATH, header=None)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sx9RwTFyC7Cy"
   },
   "source": [
    "Nachdem das Vokabular eingelesen und die Lemmatizer/Tokenizer initialisiert wurden, k√∂nnen die ben√∂tigten Funktionen erneut definiert werden. Nachdem die Daten abgespeichert wurden, verlieren die Referenzen die G√ºltigkeit. Die Referenzen zu Variablen und Methoden kann nur bestehen wenn alle ben√∂tigten Paramerter erneut definiert werden. Im folgenden Schritt wird die Score-Funktion erneut definiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dssTfOqSC7Cy"
   },
   "outputs": [],
   "source": [
    "def score(y_true, y_pred):\n",
    "    y_pred = np.rint(y_pred)\n",
    "    return f1_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWe1gEkqC7Cy"
   },
   "source": [
    "Zudem m√ºssen die Vorbereitungsfunktionen zum Entfernen der Punktuationen und dem Lemmatisieren definiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "l_LXzdP1C7Cy"
   },
   "outputs": [],
   "source": [
    "def lemmatize_words(comment):\n",
    "    word_list = pos_tag(tokenizer.tokenize(comment))\n",
    "    lemmatize_words = []\n",
    "    for word, tag in word_list:\n",
    "        wtag = tag[0].lower()\n",
    "        if wtag in ['a', 'r', 'n', 'v', 's']:\n",
    "            word = lemmatizer.lemmatize(word, wtag)\n",
    "        lemmatize_words.append(word)\n",
    "    return ' '.join(lemmatize_words)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    if type(text) != str or len(text) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    clean_text = \"\"\n",
    "    for char in text:\n",
    "        if char not in string.punctuation:\n",
    "            clean_text += char\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rr3_yt_pC7Cy"
   },
   "source": [
    "Zusammengefasst werden die Vorbereitungsfunktionen in der `generate_features` Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "_THnO7WSC7Cy"
   },
   "outputs": [],
   "source": [
    "def generate_features(sentence):\n",
    "    sentence = lemmatize_words(sentence)\n",
    "    sentence = remove_punctuation(sentence)\n",
    "    count_vectorizer = CountVectorizer(lowercase=True, analyzer='word', stop_words='english', ngram_range=NGRAM_RANGE, vocabulary=vocabulary.iloc[:, 0].values.astype('U'))\n",
    "    bag_of_words = count_vectorizer.fit_transform([sentence]) \n",
    "\n",
    "    return np.array(bag_of_words.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxHjkYSsC7Cy"
   },
   "source": [
    "Letztlich kann das Modell auf einen manuell definierten Satz angewandt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "DcCymVyfC7Cz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ergebnis der Vorhersage f√ºr 'My name is Tjark!':\n",
      "Kein Sarkasmus [0.43498561]\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load('grid-search_model.pkl')\n",
    "\n",
    "SENTENCE = \"My name is Tjark!\"\n",
    "\n",
    "features = generate_features(SENTENCE)\n",
    "prediction = model.predict(features)\n",
    "prediction_rounded = np.rint(prediction)\n",
    "if (prediction_rounded[0] == 1):\n",
    "    predicted_class = \"Sarkasmus\"\n",
    "else:\n",
    "    predicted_class = \"Kein Sarkasmus\"\n",
    "\n",
    "print(\"Ergebnis der Vorhersage f√ºr '\" + str(SENTENCE) + \"':\\n\" + predicted_class + \" \" + str(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3ddee134-a7a8-47c8-8afd-e9ca30ffbc14",
     "showTitle": false,
     "title": ""
    },
    "id": "_2IHcCVAC7Cz"
   },
   "source": [
    "<a name=\"6\"></a>\n",
    "### 6. Einbindung in eine Web-Anwendung\n",
    "Zus√§tzlich zur Nutzung √ºber den Code haben wir einen Web-Anwendung implementiert, die das Testen des Modells vereinfacht. Hierzu verwenden wir als Basis eine Express.js Applikation, die eine einfache HTML-Seite im JADE-Template-Format rendert. Innerhalb des Templates ist eine Methode definiert, die auf Knopfdruck einen Text, den der Nutzer eingegeben hat, an den Webserver sendet. Das senden findet klassisch √ºber einen AJAX-Request statt. Das Backend startet im Anschluss einen sog. `child_process`. Hierbei handelt es sich um einen gekapselte Shell, die ein `.sh` Skript ausf√ºhrt. Dieses √ºbernimmt die Kommunikation mit dem virtuellen Environment von Python. Der `child_process` wartet √ºber das Skript auf einen Antwort im JSON-Format von dem Python Skript. Das Python Skript l√§dt das Vokabular und das Modell (dieser Prozess sollte in Zukunft verbessert werden). Anschlie√üend wird das Modell mit dem Text aus der Textbox ausgef√ºhrt und das Ergebnis zur√ºckgegeben. Der `child_process` wird automatisch beendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df17b4aa-0859-4f1e-922f-148db8786e76",
     "showTitle": false,
     "title": ""
    },
    "id": "jQ9iKyQlC7Cz"
   },
   "source": [
    "<a name=\"7\"></a>\n",
    "### 7. Projektverlauf und √Ñnderungen\n",
    "In der folgenden Beschreibung erhalten Sie eine ausf√ºhrliche, zeitliche Zusammenfassung von unserem Projektablauf.\n",
    "\n",
    "__Woche vom 02.05 - 08.05.2022__\n",
    "\n",
    "Begonnen wurde damit, sich √ºber m√∂gliche Datens√§tze f√ºr das Tranieren und Testen des Modells zu informieren. Dabei wurden drei m√∂gliche Datens√§tze gefunden: iSarcasm, Sarcasm on Reddit und News Headlines Sarcasm. Schlussendlich haben wir uns f√ºr den Datensatz von Sarcasm on Reddit entschieden. Des Weiteren wurde sich die Bedeutung und der Aufbau von NGrams angeschaut und ein Skript geschrieben, welches die ersten NGrams aus einer vorgebenen Anzahl von Daten erstellt. Um sich auf den Kurzvortrag vorzubereiten wurden sich die Grundlagen zum Kernel-Trick, Ridge Regression und im Allgemeinen zur Kernel Ridge Regression angeeignet.\n",
    "\n",
    "__Woche vom 09.05 - 15.05.2022__\n",
    "\n",
    "In dieser Woche wurden die Folien des Kurzvortrages fast fertig gestellt. Es musste nur noch auf die Laufzeit eingegangen werden und die ganze Methode mit einem geeigneten Beispiel veranschaulicht werden. Es wurde eine eine Web-App programmiert, die sp√§ter beim finalen Vorstellen live das Ergebnis zeigt. Dabei wird das entsprechende Python-Skript mit dem Modell aufgerufen, welches die Ergebnisdaten der Web-App √ºbergibt. Um nicht nur mit NGrams zu arbeiten wurden beim Datensatz noch die Wahrscheinlichkeiten der Max, Min und Mean des TFIDF Wert aufgenommen.\n",
    "\n",
    "__Woche vom 16.05 - 22.05.2022__\n",
    "\n",
    "Es wurde das erste Modell gebaut und getestet. Dabei wurden als Features nur die unterschiedlichen TFIDF-Werte (Max, Min, Mean) genutzt. Die Daten sind dabei gelabelt gewesen, wobei 1 dabei f√ºr Sarkasmus steht. Dabei wurde sich auch nochmal um die m√∂gliche semantische Vergleichbarkeit von S√§tzen besch√§ftigt. Zusammengefasst war das erste Modell nicht √ºberzeugend, wobei wir uns in der darauffolgenden Woche nochmal mit weiteren m√∂glichen Features besch√§ftigt haben.\n",
    "\n",
    "__Woche vom 23.05 - 29.05.2022__\n",
    "\n",
    "In der vierten Woche haben wir nochmal eine grobe √úbersicht von dem geplanten Aufbau des Modells erstellt. Dabei soll auf dem eingegebenen Text Stemming, NGrams und ein Vokabular angewendet, um das bestm√∂gliche Ergebnis zu bekommen. Au√üerdem wurden sich weitere Gedanken √ºber m√∂gliche Features gemacht. Die enthaltenen Smileys k√∂nnten im Zusammenhang mit dem inhaltlichen Content Aufschluss √ºber Sarkasmus geben. Dabei w√ºrde √ºber den Content definiert werden, ob der Inhalt eines Kommentars positiv, negativ der neutral ist. Zus√§tzlich wurde die einfache Bag of Words Methode ohne Stemming ausprobiert. Dabei wurde mit dem Ridge Classifier auf 50% mit einer Abweichung von +-10 erreicht.\n",
    "\n",
    "__Woche vom 30.05 - 05.06.2022__\n",
    "\n",
    "Wir haben in dieser Woche mit dem Notebook angefangen, welches unseren kompletten Projektverlauf und das erarbeitete Modell beschreibt. Zudem wurde die Pr√§sentation f√ºr den Kurzvortrag vorbereitet, welche am Dienstag vorgestellt wurde. Dabei wurde die allgemeine Methode der Kernel Ridge Regression n√§her erl√§utert.\n",
    "\n",
    "__Woche vom 06.06 - 12.06.2022__\n",
    "\n",
    "Nachdem die Methode der Kernel Ridge Regression vorgestellt wurde, wurde sich wieder intensiver um das Projekt gek√ºmmert. Dabei wurde weiter am Notebook bzw. der Dokumentation geschrieben. Au√üerdem wurde dem Modell ein Vokabular hinzugef√ºgt. Dabei hatte das Vokabular im ersten Versuch einen Umfang von 27.000 W√∂rtern. Jedoch war das Training ziemlich zeitintensiv und es wurde am Ende ein Ergebnis von 39% erzielt. Um die besten Parameter herauszufinden wurde auch Hyperparametertuning angewendet.\n",
    "\n",
    "__Wochen vom 13.06 - 26.06.2022__\n",
    "\n",
    "Durch weitere √Ñnderung der Parameter durch das Hyperparametertuning und einigen Anpassungen im Vokabular kommt das Modell derzeit auf ca. 60%. Im Fokus stand jedoch in den Wochen die Dokumentation, die das Modell und den allgemeinen Projektverlauf beschreibt. Dabei wird jeder Schritt bei der Entwicklung und der Anpassung des Modells beschrieben und grundlegende Begriffe (z.B. Kernel-Trick oder Ridge Regression) definiert."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "L&SC_√úbung_SS2022",
   "notebookOrigID": 4378794509398995,
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [],
   "name": "L&SC_UÃàbung_SS2022.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c430a503694e6fed73ff44083efba88bedc6f1290fd7f1bbd6dc6d0c271a396a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
